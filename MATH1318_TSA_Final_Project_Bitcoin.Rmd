---
title: "MATH1318 Final Project: Bitcoin Time Series"
author: "Margaret Cuddihy (s3608125),	Elleni Toumpas (s3708633) and	Samuel Holt (s3381728)"

date: "29 April 2019"
output:
      pdf_document:
      toc: true
      toc_depth: 3
    
    
---

\centering
\raggedright
\newpage
\tableofcontents
\newpage


# Introduction

Bitcoin is a form of electronic cash and is popular for being the first cryptocurrency. It was invented in 2009 and is a decentralized digital currency with transactions recorded by ‘miners’, a process completed through the use of computer processing power (Calvery, 2013). Bitcoin, and cryptocurrency as a whole, is criticised for many factors, its high volatility being one of them. Bitcoin experiences many cycles bubbles and busts, and to be able forecast these trends could help in making decisions before investing or selling bitcoin. Taking into account the daily closing price of bitcoin from the 27th of April 2013 to the 24th of February 2019, sourced from coinmarketcap.com we can build a timeseries model and forecast the next 10 days with high accuracy.

## Outline of Project

The aim of this project is to use the statistical analysis methods covered in MATH1318: Time Series Analysis in order to fit an appropriate model to the Bitcoin time series which will allow us to predict a 10 day forecast. The true values of the next 10 days in the series are known, so the predictive ability of the model will be assessed using the Mean Absolute Scaled Error (MASE) method as will the MASE of the fitted model. 

Firstly, we will import and preprocess the data so it can be converted into a time series object. We will then plot the series and provide an initial inspection of the trend, seasonality, intervention points, change in variance and behaviour of the series. We will also calculate the autocorrelation of the series and examine the Autocorrelation Function (ACF), Partial Autocorrelation Function (PACF) and first lag scatter plot. From this initial inspection, we will highlight key aspects of the series and explain how these will guide our modelling strategy. 

In the next section, we noticed the presence of a trend in the series. This indicates the application of Box Cox transformation and the implementation of differencing of the series to achieve stationarity. The level of differencing will provide the d value of the ARIMA model. We will then model the mean trend of the series through ARIMA modelling, using the ACF, PACF, Extended Autocorrelation Function (EACF) and the Bayesian Information Critereon (BIC) Table to identfy potential orders of p and q variables. The potential models will be fitted and subjected to statistical testing, whereupon the significance of the coefficients and the behaviour and heteroskedasticity of the model residuals will be assessed. Additionaly, we will use Akaike Information Crieteron (AIC) and BIC scores in order to rank the given models. These methods will be used to filter out irrelevant models and possible select the best performing models. 

We suspect this series has changing variance. This assumption will be tested and verified using statistical tests and upon validation, a GARCH component will be added to our model. The GARCH orders will be estimated using the transformed residuals of the ARIMA series. The potential GARCH models will be fitted and the residuals will be inspected. Again, we will use AIC to rank the potential models and select the GARCH model with the lowest score. 

Once the ARIMA and GARCH components of our final model have been fitted to the data, we will estimate the 10 day forecast of Bitcoin closing prices. Because we will have transformed and differenced the time series, our forecast will be reverse-transformed and inverse-differenced back to the scale of the original series. We will then assess the accuracy of our prediction using the MASE method.

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages
```{r message=FALSE, warning=FALSE}
library(stringr)
library(TSA)
library(fUnitRoots)
library(lmtest)
library(tseries)
library(mvtnorm)
library(rugarch)
library(readxl)
```

```{r Appendix, include=FALSE}

acf.pacf.f <- function(x, title1, title2) {
  par(mfrow = c(1,2))
  acf(x, main = title1, cex.main=0.8)
  pacf(x, main = title2, cex.main=0.8)
  par(mfrow = c(1,1))}

plot.compare.f <- function(x1, x2, title1, title2){
  par(mfrow=c(2,1))
  plot(x1, type = "l", main = title1, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  plot(x2, type = "l", main = title2, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  par(mfrow=c(1,1))
}

norm.f <- function(model, titlehist, titleqq)
         {par(mfrow=c(1,2))
            hist(model, main = titlehist, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
            qqnorm(model, main = titleqq, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
            qqline(model, col = 2, lwd = 1, lty = 2)
            print(shapiro.test(model))
            par(mfrow=c(1,1))}

p.q.orders <- function(x, titleA, titleP, ar.max, ma.max, nar, nma)
              {acf.pacf.f(x, titleA, titleP)
              eacf(x, ar.max = ar.max, ma.max = ma.max)
              res = armasubsets(y = x, nar = nar, nma = nma, ar.method = 'yule-walker')
              plot(res)}

arima.test <- function(model, fig_num){
  res <- rstandard(model)
  par(mfrow = c(2,2))
  plot(res, type = 'o', ylab = 'Standardised residuals', main = paste0("Figure ",as.character(fig_num),"A: Time series plot of standardised residuals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  abline(h = 0)
  hist(res, main = paste0("Figure ",as.character(fig_num),"B: Histogram of standardised residuals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  qqnorm(res, main = paste0("Figure ",as.character(fig_num),"C: QQPlot of standardised residuals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  qqline(res, col = 2, lwd = 1, lty = 2)
  acf(res, main = paste0("Figure ",as.character(fig_num),"D: ACF of the Standardised Resiudals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  par(mfrow = c(1,1))
  tsdiag(model, gof = 15, omit.initial = F, main="E:")
  print(shapiro.test(res))
}

arima.garch.residual.test <- function(model){
  par(mfrow = c(1,2))
  plot(model, which = 8)
  plot(model, which = 9)
  par(mfrow = c(1,1))
  plot(model, which = 10)
}

# Obtained from the MATH1318 Canvas page for Module 7
sort.score <- function(x, score = c("bic", "aic")){
  if (score == "aic"){
    x[with(x, order(AIC)),]
  } else if (score == "bic") {
    x[with(x, order(BIC)),]
  } else {
    warning('score = "x" only accepts valid arguments ("aic","bic")')
  }
}

# Capture AIC scores and sort score order
sort.aic.garch <- function(models){
  scores <- as.data.frame(matrix(nrow=length(models), ncol=2))
  colnames(scores) <- c("Model","AIC")
  for (i in 1:length(models)){ 
    scores$Model[i] <- models[i]
    scores$AIC[i] <- infocriteria(get(models[i]))[1] 
  }
  scores <- scores[order(scores$AIC),]
  rownames(scores) <- 1:nrow(scores)
  return(scores)
}

upperbound <- function(value, sigma){
  return(value + 1.96*sqrt(sigma))
}

lowerbound <- function(value, sigma){
  return(value - 1.96*sqrt(sigma))
}

plot.reverse.forecast <- function(t.series, forc, n.ahead, title = NULL, zoomed.date = NULL, fig_num = NULL){
  
  forecasts <- forc@forecast$seriesFor
  f <- frequency(t.series)
  
  # Store all forecast values
  forecast.values <- as.data.frame(matrix(nrow=(n.ahead+1), ncol=9))
  columns <- c('time','values.f', 'values.f.invd', 'values.f.invd.exp','sigma', 'sigma.invd', 'sigma.invd.exp',
             'upper.values.f', 'lower.values.f')
  colnames(forecast.values) <- columns
  
  # Store forecast time
  forecast.values$time <- seq(time(t.series)[length(t.series)]+(1/f), (time(t.series)[length(t.series)]+(1/f))+(n.ahead*1/f), by=(1/f))
  
  # Store forecast values
  forecast.values$values.f <- c(forecasts,NA)
  
  # Store forecast inverse difference values
  forecast.values$values.f.invd <-  diffinv(as.vector(forecasts), xi=log(t.series[length(t.series)]))
  
  # Store forecast inverse exponent values
  forecast.values$values.f.invd.exp <- exp(forecast.values$values.f.invd)
  
  # capture forecasted sigma values
  forecast.values$sigma <- c(as.vector(forc@forecast$sigmaFor),NA)
  
  # inv difference for forecasted sigma values
  forecast.values$sigma.invd <- diffinv(forecast.values$sigma[1:10], xi=log(sd(t.series)))
  
  # exp for forecasted sigma values
  forecast.values$sigma.invd.exp <- exp(forecast.values$sigma.invd)
  
  # Upper and lower values of forecasted values
  forecast.values$upper.values.f <- upperbound(forecast.values$values.f.invd.exp, forecast.values$sigma.invd.exp)
  forecast.values$lower.values.f <- lowerbound(forecast.values$values.f.invd.exp, forecast.values$sigma.invd.exp)
  
  # Turn forecast values into their own timeseries
  forecast.ts <- ts(forecast.values$values.f.invd.exp[1:10], start=(time(t.series)[length(t.series)]+(1/f)),frequency = f)

  # Plot Normal series  
  if (f == 365){
    period = 'day'
  } else if (f == 12) {
    period = 'month'
  } else if (f == 4) {
    period = 'quarter'
  } else {
    period = 'period'
  }
  
  plot(x = t.series, xlim=c(time(t.series)[1],time(t.series)[length(t.series)]+((n.ahead)/f)), ylab="", xlab="Year", main=paste0("Figure ",fig_num," A: ",title, " with ",n.ahead," ",period," forecast"), type="l", cex.main=0.8)
  lines(forecast.ts, col="red", type="l")
  polygon(c((forecast.values$time[1:10]),rev(forecast.values$time[1:10])), c(forecast.values$upper.values.f[1:10],rev(forecast.values$lower.values.f[1:10])), col=rgb(0,0,0.6,0.2), border=FALSE)
  
  if (!is.null(zoomed.date)){
     # Plot Zoomed Series
    plot(x = t.series, xlim=c(zoomed.date,time(t.series)[length(t.series)]+((n.ahead)/365)), ylim=c(3500,4200), ylab="", xlab="Year", main=paste0("Figure ", fig_num," B: ",title," with ",n.ahead," ", period, " forecast ", "(zoomed plot commencing at " , zoomed.date,")"), type="l", cex.main=0.8)
    lines(forecast.ts, col="red", type="l")
    polygon(c((forecast.values$time[1:10]),rev(forecast.values$time[1:10])),
            c(forecast.values$upper.values.f[1:10],rev(forecast.values$lower.values.f[1:10])), col=rgb(0,0,0.6,0.2),
            border=FALSE)
  }
} 

MASE <- function(observed , fitted ){
  # observed: Observed series on the forecast period
  # fitted: Forecast values by your model
  Y.t = observed
  n = length(fitted)
  e.t = Y.t - fitted
  sum = 0 
  for (i in 2:n){
    sum = sum + abs(Y.t[i] - Y.t[i-1] )
  }
  q.t = e.t / (sum/(n-1))
  MASE = data.frame( MASE = mean(abs(q.t)))
  return(list(MASE = MASE))
}

```

# 1. Preprocessing 

Upon inspection of the data, we noticed two things that needed to be addressed. Firstly, the data contains two columns, the first column including dates for each observation. As we planned to convert the data into a time series object, the first column was superfluous and was therefore dropped. Secondly, we found that a few of the daily closing price values were over 1,000 and were recorded with commas. This resulted in NAs when we initially converted the data to a time series object. To prevent this, we simply removed the commas from the data. The time series begins on 27/04/2013 and was given a frequency of 365 as this is a daily series.

## Import Data
```{r}
# setwd("C:/Users/marga/OneDrive/Documents/Uni/Time Series Analysis/Final Project") # Meg's WD
# setwd("~/STUDYING/COURSES/MASTERS-ANALYTICS/03-SEMESTER-01-2019/TIME-SERIES-ANALYSIS/MATH1318_Final_Project") # Elleni's
setwd("C:/Users/samgh/Desktop/Masters of Statistics and Operations Research/Year 2/Sem 1/Time Series Analysis/Assignments/Final Assignment") # Sam's
bitcoin <- read.csv("Bitcoin_Historical_Price.csv", header = TRUE, stringsAsFactors = FALSE)
```

## Inspect the Data
```{r}
dim(bitcoin)
head(bitcoin,2)
bitcoin <- bitcoin[,2] #Only need second column 
```

## Convert to Time Series Object
```{r}
bitcoin <- as.numeric(gsub(",", "", bitcoin)) #Remove commas for thousands and convert to numeric 
which(is.na(bitcoin)) #Check no missing values - none found
bitcoin.ts <- ts(bitcoin, start = c(2013, 4, 27), frequency = 365 )
```

# 2. Intial Inspection of Bitcoin Historical Prices Series

Here we plot and inspect the time series in order to gain an initial understanding of its behaviour. 

## Plot Time Series

```{r, fig.asp = 0.5}
par(mfrow = c(1,1))
plot(bitcoin.ts, ylab = "Daily Closing Price of Bitcoin (USD)", main = "Figure 1: Bitcoin Historical Prices (2013-2019)", cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
```

### Visual Inspection of the Bit Coin Time Series (Figure 1):

1. Trend: There is a exponential increase in the trend until mid-2017 and then it steadily declines for the rest of the series. This will be confirmed using an Augmented Dickey-Fuller Test of stationarity. 

2. Seasonality: There are no seasonally repeating patterns evident in the data. If we miss any seasonal behaviour in the initial inspection, it will be apparently in the ACF and PACF plots. 

3. Changing Variance: Changing variance is very clear. Initially, variance in the closing price value is small with little variance until early 2017 and then it dramatically increases in volitlity. It appears to decrease in variation again at the very end of the series. We know financial data is sensitive to shocks so this is expected (Petrica et al, 2016). Research was conducted to identify the potential causes of these shocks, which included tightening regulation on Bitcoin trading (Chohan, 2017). 

4. Intervention Point: Through the research conducted on the exogenous events impacting the series, we identified a number of potential intervention points, such as the peak and sudden crash in December 2017 and 'big players' banning bitcoin advertisements such as Facebook and Google (Bambrough, 2018) soon after the crash. However, we deemed that none of the shocks were so great as to completely disrupt the behaviour of the series. 

5. Autocorrelation Behaviour: Both autocorrelation and moving average behaviour seem apparent. For autocorrelation behaviour, we can see instances where consecutive observations closely follow each other. For moving average, there are also instances where the data seems to sharply fluctuate. Overall, the behaviour is difficult to see because of the changing variance. 

## Check Autocorrelation between t and t-1

Firstly, we will confirm there is autocorrelation in the series. If there is no significant autocorrelation, it may be sufficient to fit the series using a simple deterministic model. However, given this a financial data time series, this is unlikely to be the case. 

#### Scatter plot of current time point values and their first lag

```{r echo=TRUE, fig.asp = 0.4}
plot(y = bitcoin.ts, x =zlag(bitcoin.ts), ylab='Bitcoin Closing Price (Yt)', xlab='Bitcoin Closing Price (Yt-1)', main = "Figure 2: Scatter plot of current bitcoin series and the previous lag", cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
abline(0,1, col = "red")
```

#### Visual Inspection of Figure 2:

1. As expected, the scatter plot shows there is a clear positive relationship between current time point values and the value of the previous lag.

2. The scatter plot also supports our assumption that there is changing variance in the series. The smaller values of the series are very tightly configured around the 45 degree line, while larger values are more spread. 


#### Calculate the correlation coefficient between current time point values and their first lag

```{r echo=TRUE}
y = bitcoin.ts               
x = zlag(bitcoin.ts)         
index = 2:length(x)    
cor(y[index],x[index])
```

#### Correlation Finding: 

The correlation coefficient calculated is 0.998, indicating extremely strong positive correlation between the current time point value and its first lag. 

Because of the strong autocorrelation, a simple deterministic trend model will not be a suitable fit for this series, as it will be inadequate in capturing the autocorrelation structure. Therefore, we must proceed with an ARIMA model. Further, the initial plot of the time series (Figure 1) and scatter plot of first lags (Figure 2) indicate there is an issue of changing variance so it is likely we will need to include a GARCH component to the final model as well. 

### ACF and PACF of the Time Series

We have established there is autocorrelation in the data. The Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PACF) of the series will provide further insight into the particular behaviour of this autocorrelation. 

```{r, fig.asp = 0.4}
acf.pacf.f(bitcoin.ts,  "Figure 3a: ACF of Bitcoin \n Historical Prices", "Figure 3b: PACF of Bitcoin \n Historical Prices")
```

#### Inspection of ACF (Figure 3a) and PACF (Figure 3b) of the Bitcoin Historical Prices Time Series:

The ACF in Figure 3a shows a large number of highly significantly lags, exhibiting a slowly decaying pattern. The PACF in Figure 3b shows one highly signficant lag at the first lag. These patterns displayed in these plots strongly suggest there is non-stationarity in the data. Not-stationarity is likely to be the cause of the changing trend behaviour identified in the initial inspection of the time series plot (Figure 1). This must be addressed before the data can be modelled.


# 2. Transformation and Differencing

Before fitting the model, any non-stationarity in the series must be reduced. To do this, we will use the Box-Cox method to first transform the data and attempt to stablise the variance. We will then test for stationarity using the Augmented Dickey-Fuller (ADF) Unit-Root Test. Once we have confirmed non-stationarity in the transformed data, we will take the difference of the time series and repeat the ADF test until non-stationarity is achieved. The transformed and differenced data is what we will use in our ARIMA modelling, with the d order equivalent to the number of differences required to achieve stationarity. 

## Apply a Box-Cox Transformation 

All values of the series are above 0 so we can directly proceed with a Box-Cox Transformation. First we will identify the optimal transformation parameter lambda.
 
```{r, fig.asp = 0.4}
bitcoin.bc <- BoxCox.ar(bitcoin.ts, method = 'yule-walker')
bitcoin.bc$ci
```

The optimal lambda is 0, indicating a log transformation on the bitcoin time series is most appropriate. 

```{r}
bitcoin.log <- log(bitcoin.ts)
plot.compare.f(bitcoin.ts, bitcoin.log, "Figure 4a: Original Bitcoin Historical Prices (2013-2019)", "Figure 4b: Log-Transformed Bitcoin Historical Prices (2013-2019)" )
```

### Visual Inspection of the Log Transformed Series Plot

The log transformation of the series has removed some of the drastic change in variance that occurs around 2017, though there is still some  volatility clustering apparent. The plot in Figure 4b now clearly shows an upward trend apparent in the data, further supporting our expectation of non-stationarity in the data.

### Check whether the log transformation has improved the normality of the series.

```{r, fig.asp = 0.4}
norm.f(bitcoin.ts, "Figure 5a: Historgram of Log Bitcoin Prices", "Figure 5b: QQ Plot of Log Bitcoin Prices")
norm.f(bitcoin.log, "Figure 5c: Historgram of Log Bitcoin Prices", "Figure 5d: QQ Plot of Log Bitcoin Prices")
```

### Result of Normality Tests for Log-Transformed Series

The normality of the distribution has considerably improved. The histogram of the transformed data (Figure 5c) is far less postiviely skewed than the original data (Figure 5a) so there has been some improvement to the distribution. The QQ Plot (Figure 5c) shows that the data now more closely resembles the red reference line of normality, though there is still signficant deviation, particularly around the tails. The thickness of the tails in the QQ Plot is an indication of changing variance. Further, the Shapiro-Wilk test result supports that the data is not normally distributed, even after the log transformation.

### ACF and PACF of the Log-Transformed Series

```{r, fig.asp = 0.4}
acf.pacf.f(log(bitcoin.ts), "Figure 6a: ACF of \n Log Bitcoin Prices", "Figure 6b: PACF of \n Log Bitcoin Prices")
```

### Visual Inspection of the Log Transformed Series ACF and PACF

Although the distribution of the data was somewhat improved by the log transformation, the ACF (Figure 6a) and PACF (Figure 6b) of the log-transformed series still strongly indicate that the series is non-stationary. An upward trend was also obvious in the plot of the transformed data (Figure 5a) This will be confirmed using a unit-root test. If confirmed, differencing of the series will be used to render the series stationary. 

## Augmented Dickey-Fuller Unit-Root Test of Trend Stationarity

The null hypothesis of the test is that the data is trend non-stationary; that is, the data is not stationary but can be rendered so through differencing. The alternative hypothesis is that the data is stationary. We will use the significance level of 95%, so a p-value less than 0.05 will result in a rejection of the null hypothesis. 

```{r}
ar(diff(bitcoin.log)) #Order of lags select = 31
adfTest(bitcoin.log, lags = 31, type = "nc", title = NULL,description = NULL)
```

### Results of the ADF Test

The p-value of the ADF unit-root test is much higher than 0.05, leading the the failure to reject the null hypothesis of trend non-stationarity. We can proceed with differencing the series to remove the trend. 

## First Differencing 

```{r, fig.asp = 0.8}
#bitcoin.r <- diff(bitcoin.log)*100  #Multiplying by 100 gives the percentage change of prices 
bitcoin.r <- diff(bitcoin.log)
plot.compare.f(bitcoin.log, bitcoin.r, "Figure 7a: Log-Transformed Bitcoin Historical Prices (2013-2019)", "Figure 7b: First Difference of Log-Transformed Bitcoin Historical Prices (2013-2019)")
```

### Results of First Differencing - Plot Inspection

Taking the first difference of the log-transformed Bitcoin Historical Prices series has appeared to largely remove the positive trend from the series. Now the data appears to fluctuate around a mean of zero. The volatility clustering is clearly evident, with large variance clusters occuring around midway through 2013 and 2017 in particular. This indicates that a GARCH component may need to be included in our final model as forecasting will need to take into account conditional variance. 

## Augmented Dickey-Fuller Unit-Root Test of Trend Stationarity for the First Difference

Stationarity will be confirmed with an ADF unit-root test applied to the first difference of the log-transformed series. 

```{r}
ar(diff(bitcoin.r)) #Order of lags select = 32
adfTest(bitcoin.r, lags = 32, type = "nc", title = NULL,description = NULL)
```

The p-value of the ADF test when applied to the first difference of the log-transformed series is less than 0.01. This is statistically signficant enough to reject the null hypothesis and conclude that the series has be rendered stationary. 

This combination of taking the log-transformation and first difference of the series is also commonly referred to as the returns of the series. The returns indicate the percentage movement of prices for each day (hence, the object bitcoin.r was multiplied by 100). We will proceed with determining the ARIMA orders of the Bitcoin Historical Prices series using the returns.

# 3. Determining the Potential Orders of an ARIMA Model

An ARIMA(p,d,q) model will be fitted and used to predict future mean levels of Bitcoin prices. In Section 2 of the report, it was determined that the differencing order will be 1 as taking the first difference successfully rendered the time series stationary (d = 1). The potential orders of p and q will be estimated using the ACF, PACF, Extended ACF (EACF) and Bayes Information Critereon (BIC) Table of the returns series of Bitcoin Historical Prices. 

`
```{r, fig.asp = 0.4}
p.q.orders(bitcoin.r, "Figure 8a: ACF of Returns", "Figure 8b: PACF of Returns", 10, 10, 13, 12)
```


### ACF and PACF Estimated Orders of p and q

Taking the first difference of the Bitcoin prices series has removed the slow decaying pattern of highly signficant lags in the ACF (Figure 8a). The ACF and PACF (Figure 8b) show a number of signficant lags, but due to the changing variance in the series, the interpretation of the likely orders of p and q is not clear. For the ACF, there are definitely 2 signficant lags, possibly 4 or even 6 that could be considered candidate values for q. Similarly in the PACF, there are definitely two signficant lags towards the start and possibly 4 or 6 signficant lags. The changing variance is also likely to be the cause of higher order lags further back in the series which are less clearly significant. 

Possible ARIMA models: {ARIMA(2,1,2), ARIMA(2,1,4), ARIMA(2,1,6), ARIMA(4,1,2), ARIMA(4,1,4), ARIMA(4,1,6), ARIMA(6,1,2), ARIMA(6,1,4), ARIMA(6,1,6)}

### EACF Estimated Orders of p and q

On initial inspection of the EACF plot, it appears the highest left vertex is at (0,0) indicating that the returns series is a white noise series, but because there has been evidence of changing variance in the series, it is likely that the EACF output is "fuzzy"; that is, the location of the vertex is not clear. Another possible vertex point exists at (5,5), (6,5), (5,6) and (6,6) so we will take these as possible orders. 

Possible ARIMA models: {ARIMA(5,1,5), ARIMA(5,1,6), ARIMA(6,1,5), ARIMA(6,1,6)}

### BIC Table Estimated Orders of p and q

The BIC table indicates the models with the low BIC values tend to have AR orders of 6 and 11 and MA order of 11.  The BIC Table tends to yield quite high orders, but since the series has over 2000 observations these orders are not unreasonable.  

Possible ARIMA models: {ARIMA(6,1,0), ARIMA(6,1,11), ARIMA(11,1,11)}


# 4. Testing Potential ARIMA Models

The final set of models to be tested is:

{ARIMA(2,1,2), ARIMA(2,1,4), ARIMA(2,1,6), ARIMA(4,1,2), ARIMA(4,1,4), ARIMA(4,1,6), ARIMA(5,1,5), ARIMA(5,1,6), ARIMA(6,1,0), ARIMA(6,1,2), ARIMA(6,1,4), ARIMA (6,1,5) ARIMA(6,1,6), ARIMA(6,1,11), ARIMA(11,1,11)}

The methodology for chosing the best ARIMA model will firstly involve fitting each set of ARIMA orders to the data using the Maximum Likelihood Estimation (MLE) method and inspecting the coefficients generated by the model to see if they are shown to be statistically significant. Models with all or most coefficients significant will be prioritised. We will also inspect the residuals of each model by inspecting the plot of the standardised residuals, the ACF of the standardised residuals and by calculating the p-values for the Ljung-Box statstic at each lag. In the ACF of the residuals, we will be looking to see if the model has sufficiently captured the autocorrelation of the series, leaving only white noise. Models generating ACFs with no significant lags will be prioritised. For the Ljung-Box test, the null hypothesis is that there is no autocorrelation left in any part of the residuals. Therefore we will be looking for p-values greater than the significance threshold of 0.05 to indicate that the model has suffcienctly captured autocorrelation. Models will p-values higher than 0.05 at all lags will be prioritised. 

```{r eval=FALSE, include=FALSE}
# **<CAN WE DELETE THIS?? IT DOESN'T SEEM TO BE USED - ELLENI >**
# model.list <- list(c(2,1,2), 
#                   c(2,1,4), 
#                   c(2,1,6), 
#                   c(4,1,2),
#                   c(4,1,4),
#                   c(4,1,6),
#                   c(5,1,5),
#                   c(5,1,6),
#                   c(6,1,0),
#                   c(6,1,2),
#                   c(6,1,4),
#                   c(6,1,5),
#                   c(6,1,6),
#                   c(6,1,11),
#                   c(11,1,11)
#                   )
# 
# modelEstimation <- myCandidate(bitcoin.r, orderList = model.list, methodType = "ML")
# modelEstimation
# modelEstimation$IC
# residual.analysis(modelEstimation$model[[1]])
# residual.analysis(modelEstimation$model[[2]])
# residual.analysis(modelEstimation$model[[3]])
# residual.analysis(modelEstimation$model[[4]])
# residual.analysis(modelEstimation$model[[5]])
# residual.analysis(modelEstimation$model[[6]])
# residual.analysis(modelEstimation$model[[7]])
# residual.analysis(modelEstimation$model[[8]]) #residuals uncorrelated (5,1,6)
# residual.analysis(modelEstimation$model[[9]])
# residual.analysis(modelEstimation$model[[10]]) 
# residual.analysis(modelEstimation$model[[11]])
# residual.analysis(modelEstimation$model[[12]]) #residuals uncorrelated (6,1,5)
# residual.analysis(modelEstimation$model[[13]]) #residuals uncorrelated (6,1,6)
# residual.analysis(modelEstimation$model[[14]]) #residuals uncorrelated (6,1,11)
# residual.analysis(modelEstimation$model[[15]]) #residuals uncorrelated (11,1,11)

```

1. Models with significant coefficients
2. Residual analysis (Ljung-Box Test)

**ARIMA(2,1,2)**

In our first ARIMA test we explore the candidate ARIMA(2,1,2) model. In the coefficient test we can see that we have two coefficients that are statistically insignificant (with a p-value greater that the significance level of 0.05) and two values with NAs in the Standard Error and p-values of the coefficients.Insignificant p-values allude to the fact that this candidate model does not capture the series well.  The NA values are caused by the square root of negative values found in the hessian matrix which is passed when fitting an ARIMA model.

In the ACF plot of Standardised Residuals (figure 9D) we still have significant ACF lags, corresponding to some autocorrelation left in the residuals. In the Ljung Box test (titled p-values for Ljung Box statistic) we have a mix of  statistically signficant and statistically insignificant lags. This deems the model as not valid.


```{r}
b.212 <- arima(bitcoin.log, order=c(2,1,2), method = 'ML')
coeftest(b.212)
arima.test(b.212, 9)
```



**ARIMA(2,1,4)**

The next canidate model, ARIMA(2,1,2), is tested below. In the coefficient test we can see only two statistically significant MA coefficients ($\theta_2$ and $\theta_4$) with the rest of the coefficient estimates being statistically insignificant. 

In the ACF plot of Standardised Residuals (figure 10D) we again still have significant ACF lags with the Ljung Box test (titled p-values for Ljung Box statistic) having mainly statistically insignificant lags. But due to the significant lags at 11 onward, this model is also not viable.

```{r}
b.214 <- arima(bitcoin.log, order=c(2,1,4), method = 'ML')
coeftest(b.214)
arima.test(b.214, 10)
```



**ARIMA(2,1,6)**

With the candidate model ARIMA(2,1,2) half the coefficients are statistically significant ($\phi_1$, $\phi_2$, $\theta_1$ and $\theta_2$). This could be a symptom of overfitting, which will be investigated if this model proves to be significant.

In the ACF plot of Standardised Residuals (figure 11D) we only have late significant ACF lags.

The Ljung-Box test (titled p-values for Ljung Box statistic) we have no lags that fall below the significant level, meaning that there is no significant autocorrelation left in the lags of the residuals of this candidate model. We then deem ARIMA(2,1,6) as a candidate model.

```{r}
b.216 <- arima(bitcoin.log, order=c(2,1,6), method = 'ML')
coeftest(b.216)
arima.test(b.216, 11)
```



**ARIMA(4,1,2)**

In the candidate model ARIMA(4,1,2) we have only one statistically significant coefficient ($\phi_4$). We can see late lags in the ACF plot of residuals (plot 12D) but we also see that the Ljung-Box test on the candidate model's residuals show some lags that are statistically significant. We interpret this to mean that there is some autocorrelation left in these lags and thus reject the validity of this model.

```{r}
b.412 <- arima(bitcoin.log, order=c(4,1,2), method = 'ML')
coeftest(b.412)
arima.test(b.412, 12)
```



**ARIMA(4,1,4)**

In the candidate model ARIMA(4,1,4) we have the first candidate model with all coefficients returning statistically significant. 

In the plot of the standardised residuals (Figure 13A) we see points hovering over a mean value of 0, however they are not independantly distributed. We still changes in variation across the residual plot. 

In the ACF plot of Standardised Residuals (figure 13D) we only have late significant ACF lags. The Ljung-Box test (titled p-values for Ljung Box statistic) also has no lags that fall below the significant level, meaning that there is no significant autocorrelation left.

This candidate model seems promising in consideration for the capturing the ARIMA component of the Bitcoin series.

```{r}
b.414 <- arima(bitcoin.log, order=c(4,1,4), method = 'ML')
coeftest(b.414)
arima.test(b.414, 13)
```



**ARIMA(4,1,6)**

In the candidate model ARIMA(4,1,6) we only three statistically significant coefficients ($\phi_4$, $\theta_4$ and $\theta_6$)

In the ACF plot of Standardised Residuals (figure 14D) we only have late significant ACF lags with the Ljung Box test (titled p-values for Ljung Box statistic) having statistically significant lags. 

The standardised residuals display change in variance over the time series, this can be addressed using a GARCH component if the model shows no significant lags in the Ljung Box test. As displayed below, there are indeed no significant lags, deeming ARIMA(4,1,6) a candidate model.

```{r}
b.416 <- arima(bitcoin.log, order=c(4,1,6), method = 'ML')
coeftest(b.416)
arima.test(b.416, 14)
```

**ARIMA(5,1,5)** 

In the next candidate model ARIMA(5,1,5) we have a mix of results in the coefficient test. Two coefficients ($\phi_4$ and $\theta_4$) produce NA values in the test for statistical significance. One value ($\theta_3$) is statistically insignificant, leaving the rest of the coefficients statistically significant. 

In the ACF plot of Standardised Residuals (figure 15D) we yet again only have late significant ACF lags with the Ljung Box test (titled p-values for Ljung Box statistic) having statistically significant lags.

Again, change in variance is present in the standardised residual plot, deeming the possible use of a GARCH component. The Ljung Box test showsno significant lags, deeming ARIMA (5,1,5) as a candidate model.

```{r}
b.515 <- arima(bitcoin.log, order=c(5,1,5), method = 'ML')
coeftest(b.515)
arima.test(b.515, 15)
```


**ARIMA(5,1,6)**

Testing the hypothesised ARIMA(5,1,6) model below we find four coefficients statistically insignificant  ($\phi_4$ and $\theta_1$, $\theta_4$ and $\theta_6$). The rest of the coefficients are statistically significant.

The histogram of residuals (in Figure 16B) shows the residuals are symmetrically distributed but the shape of the bell curve is very extreme.  The QQ Plot (Figure 16C) shows very thick tails indicating changing variance which will need to be handled. The Shapiro-Wilk result is a very small p-value, indicating the rejection of the null hypothesis of normally distributed residuals. 

The ACF of the Standardised Residuals (Figure 16D) indicates no autocorrelation left in the residuals. The p-value of the Ljung-Box test is greater than 0.05, indicating that there is no signficant evidence to reject the null hypothesis that the residuals are uncorrelated. 

From the plot (Figure 16A), there is still obvious volatility clustering that can be handled by proceeding with a GARCH modelling component.

As a result we have found this candidate model a viable model for further exploration.

```{r}
b.516 <- arima(bitcoin.log, order=c(5,1,6), method = 'ML')
coeftest(b.516)
arima.test(b.516, 16)
```

**ARIMA(6,1,0)**

Testing the candidate model ARIMA(6,1,0) we find that all coefficients are statistically insignificant, except for the coefficients ($\phi_5$ and $\phi_6$). This may indicate not overfitting, but simply that most coefficients of lower orders are not useful for capturing all stochastic processes of the time series. 

Histogram of residuals (Figure 17B) shows they are symmetrically distributed but the shape of the bell curve is very extreme. The QQ Plot (Figure 17C) shows very thick tails indicating changing variance which will need to be handled.  Shapiro-Wilk result is a very small p-value, indicating the rejection of the null hypothesis of normally distributed residuals. 

The ACF of Residuals (Figure 17D) indicates there is autocorrelation left in the residuals. However the p-value of the Ljung-Box test supports the argument that the lags are not statistically significant. This allows us to deem ARIMA (6,1,0) as a candidate model.


```{r}
b.610 <- arima(bitcoin.log, order=c(6,1,0), method = 'ML')
coeftest(b.610)
arima.test(b.610, 17)
```

**ARIMA(6,1,2)**

When testing the ARIMA(6,1,2) candidate model we can see insignificant coefficients on all estimates from $\phi_3$ t0 $\phi_6$. The higher orders of $\phi$ may be an indication of overfitting. 

In the ACF plot of the Residuals (Figure 18D) we can see late autocorrelated lags. Thick tails appear in the QQ Plot (Figure 18C) and with volatility clustering in the plot of residuals (Figure 18A). The Ljung-Box test shows that all lags are statistically insignificant. This deems ARIMA(6,1,2) as a valid candidate model.


```{r}
b.612 <- arima(bitcoin.log, order=c(6,1,2), method = 'ML')
coeftest(b.612)
arima.test(b.612, 18)
```

**ARIMA(6,1,4)**

When testing the ARIMA(6,1,4) candidate model we can see insignificant coefficients in two estimates ($\phi_5$ and $\phi_6$) with $\phi_1$ and $\phi_4$ as statistically significant coefficients and coefficients with NA p-values over the rest. This implies the infinite approach to a given value or the square root of a negative value and possible overfitting with insignificant higher order values.

In the ACF plot of the Residuals (Figure 19D) we can see late autocorrelated lags, with yet again thick tails appearing in the QQ Plot (Figure 19C) and volatility clustering in the plot of residuals (Figure 19A). The Ljung-Box test shows that all lags are statistically insignificant. This allows us to deem ARIMA(6,1,4) as a valid candidate model.


```{r}
b.614 <- arima(bitcoin.log, order=c(6,1,4), method = 'ML')
coeftest(b.614)
arima.test(b.614, 19)
```


**ARIMA(6,1,5)**

In the ARIMA(6,1,5) candidate model, most coefficient estimates are signficant except for $\phi_4$, $\phi_6$ and  $\theta_4$.

In the ACF plot of the Residuals (Figure 20D) we can see only one autocorrelated lag and yet again thick tails appear in the QQ Plot (Figure 20C) with volatility clustering in the plot of residuals (Figure 20A). 

The Ljung-Box test shows that all lags are statistically insignificant, meaning the lags hold no statistically significant autocorrelaton. This allows us to deem ARIMA(6,1,5) as a valid candidate model.

```{r}
b.615 <- arima(bitcoin.log, order=c(6,1,5), method = 'ML')
coeftest(b.615)
arima.test(b.615, 20)
```

**ARIMA(6,1,6)**

In the ARIMA(6,1,6) candidate model, all coefficient estimates return a NA p-value. This indicates that upon the passing of the hessian matrix within the ARIMA model fit, negative values were involved and thus we may consider rejecting this model. We shall investigate further the residuals and lags to determine the validity of this model.

In the ACF plot of the Residuals (Figure 21D) we can see only one, maybe two autocorrelated lag and the Ljung-Box test shows that all lags are statistically insignificant, meaning the lags hold no statistically significant autocorrelaton.

Thick tails appear in the QQ Plot (Figure 21C) with volatility clustering in the plot of residuals (Figure 21A). This aspect not being captured by the model could be handled by a GARCH component, considered a later in the report.

The Ljung Box test shows no significant lags thus we deem ARIMA(6,1,6) as a statistically valid model to consider.

```{r}
b.616 <- arima(bitcoin.log, order=c(6,1,6), method = 'ML')
coeftest(b.616)
arima.test(b.616, 21)
```

**ARIMA(6,1,11)**

In the ARIMA(6,1,11) we have only six statistically significant coefficient estimates, with three coefficients with NA p-values and the rest all statistically insignificant. Although the highest order of $\phi$ is significant, all higher orders of $\theta_7$ onward do not show any statistical significance. This could indicate overfitting.

We have two lags autocorrelated found in the ACF plot of residuals in Figure 22D. The Ljung-Box test shows that all lags are statistically insignificant, meaning the lags hold no statistically significant autocorrelaton.

Thick tails appear in the QQ Plot (Figure 21C) with volatility clustering in the plot of residuals (Figure 21A). This aspect not being captured by the model could be handled by a GARCH component, considered a later in the report.

The Ljung Box test shows no significant lags thus we deem ARIMA(6,1,11) as a statistically valid model to consider.


```{r}
b.6111 <- arima(bitcoin.log, order=c(6,1,11), method = 'ML')
coeftest(b.6111)
arima.test(b.6111, 22)
```

**ARIMA(11,1,11)**

In the test on ARIMA(11,1,11) we find nine coefficient estimates to be statistically signficant and the rest are statistically insignificant. Given that the highest orders of $\phi$ and $\theta$ are significant, the insignificant variables do not indicate overfitting.

In the ACF plot of the Residuals (Figure 23D) we can see only one late significant lag with the Ljung-Box test showing that all lags are statistically insignificant, meaning the lags actually hold no statistically significant autocorrelaton.

Thick tails appear in the QQ Plot (Figure 21C) with volatility clustering in the plot of residuals (Figure 21A). This aspect not being captured by the model could be handled by a GARCH component, considered a later in the report.

The Ljung Box test shows no significant lags thus we deem ARIMA(11,1,11) as a statistically valid model to consider.

```{r}
b.11111 <- arima(bitcoin.log, order=c(11,1,11), method = 'ML')
coeftest(b.11111)
arima.test(b.11111, 23)
```


Based on this analysis, we can narrow down our selection of possible models. The models that successfully removed autocorrelation from the series were {ARIMA(2,1,6), ARIMA(4,1,6), ARIMA(5,1,5), ARIMA(5,1,6), ARIMA(6,1,0), ARIMA(6,1,4), ARIMA(6,1,5), ARIMA(6, 1, 11), ARIMA(11,1,11)}.


### AIC BIC Ranking

The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) score will be calculated for each model. The models will be ranked in descending order and the model with the lowest score will be selected.

```{r}

sort.score(AIC(b.216,b.416,b.515,b.516, b.610,b.612, b.614, b.615, b.6111, b.11111), score = "aic")
# sort.score(AIC(b.516, b.612, b.614, b.615, b.6111, b.11111), score = "bic")
AIC(b.216,b.416,b.515,b.516, b.610,b.612, b.614, b.615, b.6111, b.11111, k = log(28))
#Note that sort.score function was obtained from the MATH1318 Canvas Shell under Module 7. 

BIC(b.216,b.416,b.515,b.516, b.610,b.612, b.614, b.615, b.6111, b.11111)

#**<OUT OF INTEREST SAKE I HAVE PLACE ALL>**
# sort.score(AIC(b.212, b.214, b.216, b.412, b.414, b.416, b.515, b.516, b.610, b.612, b.614, b.615, b.616, b.6111, b.11111), score = "aic")
```


### Final ARIMA Model Chosen:

ARIMA(5,1,6) has low scores for both AIC and BIC. We also found that most of the coefficients generated by MLE for this model were statistically significant. Only AR4 was found to be insignificant, with the next coefficient, AR5, significant, so this is acceptable. Further, the ACF of the standardised residuals indicated the model sufficient captured the autocorrelation of the model, showing no significant lags. The p-value of the Ljung-Box test is greater than 0.05 for every lag, indicating that there is no signficant evidence to reject the null hypothesis that the residuals are uncorrelated. 

From the standardised residuals plot, there is still obvious volatility clustering that will need to be examined further.


### Overfitting Testing

Before we accept ARIMA(5,1,6) as the chosen model, we will use overfitting; that is, we will confirm that the next-largest model does not perform better. Since ARIMA(6,1,6) was one of the candidate models, only ARIMA(5,1,7) will be tested. 

```{r}
b.517 <- arima(bitcoin.log, order=c(5,1,7), method = 'ML') #Elleni did not get NAs
coeftest(b.517)
AIC(b.516, b.517)
arima.test(b.517, 24)
```

ARIMA(5,1,7) produced NAs as coefficient estimates. Although the models seems to have generated residuals free of autocorrelation, it also generated a higher AIC value than ARIMA(5,1,6). Therefore, there is no compelling evidence that the ARIMA(5,1,7) model performs better, so we can proceed with ARIMA(5,1,6).








# 5. Modelling Conditional Variance with GARCH

We can use ARIMA(5,1,6) to model and forecast future mean levels of Bitcoin prices. However, throughout the report so far there has been strong evidence of conditional variance (i.e. variance that changes over time). In the original time series plot (Figure 1), there was evidence of changing variance over time. Volatility clustering was even clearer in the plot of the log-transformed first difference of the series (Figure 7b). After our chosen ARIMA model was fitted, there was still volatility clustering present in the standardised residuals plot and in the the thick tails of the QQ Plot. 

In order to incorporate this conditional variance into the Bitcoin Historical Prices series modelling, we will examine two transformations of the returns residuals and estimate orders p and q of GARCH to combine with ARIMA(5,1,6) so that we can model both mean and variance of the series. 

## Confirming the Presence of ARCH

### Plot of Standardised Residuals

```{r}
bitcoin.res <- rstandard(b.516)
plot.compare.f(bitcoin.r, bitcoin.res, "Figure 25a: Plot of Returns Series ", "Figure 25b: Plot of ARIMA(5,1,6) Standardised Residuals")
```

In the plot of the residuals of the ARIMA(5,1,6) model, volatility clustering is still apparent.

### ACF and PACF of Standardised Residuals

```{r}
acf.pacf.f(bitcoin.res, "ACF", "PACF")
eacf(bitcoin.res)
```

The ACF, PACF and EACF of the residuals all indicate white noise. This can imply that the ARIMA model has perfectly captured autocorrelation but it can also indicate changing variance. 

### Normality Test of Standardised Residuals 

```{r}
norm.f(bitcoin.res, "Histogram", "QQ Plot")
```

If the ARIMA(5,1,6) model had perfectly captured all the autocorrelation structure of the series, we would end up with i.i.d. variables in the residuals. However, looking at the QQ plot, it is clear the residuals do not follow the line of normal distribution. The fat tails on either end are an indicator of conditional variance in the series. 

### McLeod-Li Test of Standardised Residuals 

```{r}
McLeod.Li.test(y = bitcoin.res, main = " Figure 12a: McLeod-Li Test Statistics", gof.lag = 30, cex.main=0.8)
```

The null hypothesis of the McLeod-Li test is there is no Autoregressive Conditional Heteroskedasticity (ARCH) in the series. All p-values of the McLeod-Li test are statistically significant, indicating that we reject the null hypothesis and conclude there is ARCH behaviour in this series. 

### ACF and PACF of Nonlinear Transformations

If the residual values are truly independent, then applying nonlinear transformations to the data should not affect the autocorrelation structure of the data. We should still see no signficant lags in the ACF and PACF as is the case for the untransformed residuals. 

```{r}

#Absolute Value Transformtaion

acf.pacf.f(abs(bitcoin.res), "Figure 11a: ACF of Abs. \n Transformed\n Standardised Residuals", "Figure 11b: PACF of Abs.\n Transformed \n Standardised Residuals")

#Squared Value Transformtaion
acf.pacf.f(bitcoin.res^2, "Figure 11c: ACF of Sqr. \n Transformed\n Standardised Residuals", "Figure 11d: PACF of Sqr.\n Transformed \n Standardised Residuals")

```

Comparing Figures 11a and 11b of the Absolute Values transformed residuals with Figures 11c and 11d of the Squared Values transformed residuals, the ACF and PACF of the different transformations are very different. Further, there are a large number of signficant lags. Therefore the assumption of i.i.d. residuals is violated. 

This is sufficient evidence to conclude there is an ARCH behaviour in the variance of the series. We can estimate a GARCH component using the EACF of the transformed residuals to improve the accuracy of our ARIMA(5,1,6) model.

## Estimate the Orders of GARCH 

### EACF of Transformed Residuals

```{r}
eacf(abs(bitcoin.res))
```

For the Absolute Values Transformed Residuals, the vertex appears to fall at (2,2).

Possible values of p: 2, 3. Possible values of q: 2, 3. Possible values of Max(p,q): 2,3

Possible Models: {GARCH(2,2) GARCH(3,2), GARCH(3,3)}


```{r}
eacf((bitcoin.res)^2)  #Slightly different result with lambda exact box-cox 
```

The vertex is a bit less clear for the Squared Values Transformed Residuals, but possibly falls at (4,4) or (4,5).

Possible values of p: 4, 5. Possible values of q: 4, 5, 6. Possible values of Max(p,q): 4, 5, 6

Possible Models: {GARCH(4,4), GARCH(5,4). GARCH(5,5), GARCH(6,4), GARCH(6,5)}


## Fit Parameter Estimates

We will fit both the ARMIA(5,1,6) and the possible GARCH orders to the data. will look for the model with the lowest AIC score. Ideally, we will see mostly significant coefficients and some improvement to the residuals as well. 

### Absolute Value Transformation

**GARCH(2,2)

```{r, fig.asp = 0.4}
a.22 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(2, 2)), mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")
m.56_22 <- ugarchfit(spec = a.22, data = bitcoin.r, out.sample = 100)
#m.56_22
m.56_22@fit$matcoef  
infocriteria(m.56_22)
arima.garch.residual.test(m.56_22)
```

**GARCH(3,2)**

```{r, fig.asp = 0.4}
a.32 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(3, 2)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")

m.56_32 <- ugarchfit(spec = a.32, data = bitcoin.r, out.sample = 100)
# m.56_32  
m.56_32@fit$matcoef  
infocriteria(m.56_32)
arima.garch.residual.test(m.56_32)
```

**GARCH(3,3)**

```{r, fig.asp = 0.4}
a.33 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(3, 3)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE), distribution.model = "norm")

m.56_33 <- ugarchfit(spec = a.33, data = bitcoin.r, out.sample = 100)
#m.56_33 
m.56_33@fit$matcoef  
infocriteria(m.56_33)
arima.garch.residual.test(m.56_33)
```


### Square Root Transformation

**GARCH(4,4)**

```{r, fig.asp = 0.4}
a.44 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(4, 4)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")
m.56_44 <- ugarchfit(spec = a.44, data = bitcoin.r, out.sample = 100)
# m.56_44 
m.56_44@fit$matcoef  
infocriteria(m.56_44)
arima.garch.residual.test(m.56_44)
```

**GARCH(5,4)**

```{r, fig.asp = 0.4}
a.54 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(5, 4)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")
m.56_54 <- ugarchfit(spec = a.54, data = bitcoin.r, out.sample = 100)
# m.56_54 
m.56_54@fit$matcoef  
infocriteria(m.56_54)
arima.garch.residual.test(m.56_54)
```

**GARCH(5,5)**

```{r, fig.asp = 0.4}
a.55 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(5, 5)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")
m.56_55 <- ugarchfit(spec = a.55, data = bitcoin.r, out.sample = 100)
# m.56_55 
m.56_55@fit$matcoef  
infocriteria(m.56_55) 
arima.garch.residual.test(m.56_55)
```

**GARCH(6,5)**

```{r, fig.asp = 0.4}
a.65 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(6, 5)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")
m.56_65 <- ugarchfit(spec = a.65, data = bitcoin.r, out.sample = 100)
# m.56_65
m.56_65@fit$matcoef  
infocriteria(m.56_65)
arima.garch.residual.test(m.56_65)
```

## Final GARCH Model Chosen

Sorting the AIC scores we can see that the GARCH(5,5) has the lowest of -3.776510. <Insert line about 'As the statistical explorations above also support this we select this as the best model' > 

```{r}
sort.aic.garch(c('m.56_22', 'm.56_32', 'm.56_33', 'm.56_44', 'm.56_54', 'm.56_55', 'm.56_65'))
```




### Plot of Estimated Conditional Variances

**< Comments about this ?? >**

```{r}
par(mfrow=c(1,1))
plot(ts((fitted(m.56_55)[,1])^2, start = c(2013, 4, 27), freq = 365),type='l',ylab='Conditional Variance',xlab='t',main="Estimated Conditional Variances of the Daily Returns", cex.main=0.8)
```



# 6. Forecasting Bitcoin Prices using ARIMA(5,1,6) + GARCH(5,5)

In forecasting, accuracy of a model is measured by the mean absolute scaled error. Capturing the forecasted values from the model and calculating the MASE with them and also the values of the fitted model will dictate the overall accuracy.

To do this we first create a dataframe for capturing the scores. The observed (real) Bitcoin values for the forecast period is imported.

```{r}
# Preparation for MASE
forecast_doc <- read_excel ('mase_tools/Bitcoin_Prices_Forecasts.xlsx')
observed <- forecast_doc$`Closing price`
mase_results <- as.data.frame(matrix(nrow=2, ncol=2))
colnames(mase_results) <- c("", "BEST MASE")
```

The chosen ARIMA/ GARCH model is fitted using the chosen orders (ARIMA(5,1,6) x GARCH(5,5)). The model is fitted using the differenced and log transformed bitcoin series.

```{r warning=FALSE}
# Fitting the ARIMA/ GARCH model
model <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(5,5), submodel = NULL, external.regressors = NULL, variance.targeting = FALSE), mean.model =   list(armaOrder = c(5,6), external.regressors = NULL, distribution.model = "norm", start.pars = list(), fixed.pars = list()))
m.fit <- ugarchfit(spec = model, data = diff(log(bitcoin.ts)), solver.control = list(trace = 0))
```




**MASE over fitted values**

Capturing the fitted values, we must inversed the first difference (using the log of the first bitcoin value in the xi parameter). Once the inverse differenced values are captured, the exponent of these values are also executed, thus also reversing the log transformation in the values. Running the MASE function with the original timeseries values and the untransformed, inversed differenced fitted values for the ARIMA/ GARCH model, we receive a MASE score of 19.43986 for the Fitted Values.

```{r warning=FALSE}
fitted.values <- fitted(m.fit)

# Inversing the difference for the fitted values
fitted.values.invdiff <- diffinv(as.vector(fitted.values), xi=log(bitcoin.ts[1]))

# Reversing the log transformation for the fitted values
fitted.values.invdiff.exp <- exp(fitted.values.invdiff)

# MASE of fitted values
mase_results[1,] <- c("Over Fitted Values",MASE(bitcoin.ts,fitted.values.invdiff.exp)$MASE$MASE)

```



**MASE over the forecasts**

Capturing the forecasted values, we again inverse the first difference (using the log of the LAST bitcoin value in the xi parameter). After this, we yet again capture the exponent of these values. We run these values with the supplied real values for the forecast period to get a MASE score of 1.26147688 for the forecasted values.

```{r warning=FALSE}
forc <- ugarchforecast(m.fit, n.ahead = 10 ,data = diff(log(bitcoin.ts)))
plot(forc, which = 1)
forecasts <- forc@forecast$seriesFor

# Inversing the difference for the forecast values
forecasts.invdiff <- diffinv(as.vector(forecasts), xi=log(bitcoin.ts[length(bitcoin.ts)]))

# Reversing the log transformation for the forecast values
forecasts.invdiff.exp <- exp(forecasts.invdiff)

# MASE of forecast values
mase_results[2,] <- c("Over Forecasts",MASE(observed,forecasts.invdiff.exp[1:10])$MASE$MASE)
```


**Plot original timeseries with inverse differenced and untransformed forecasts**

The plot of the inversed differenced and untransformed forecast are plotted below with the original timeseries. The confidence intervals are generated by capturing the sigmas (standard deviation) of the forecast values, and also by reversing the differencing and log transformations that are inherently captured in these sigma values. The upper and lower bounds for the forecasts are then calculated and turned into a polygon shape outlines the confidence intervals for the prediction. Figure XXB shows a zoomed in plot so the forecast line and confidence intervals can be seen clearly. The zoomed in series displays the series from August 2018.

```{r}
plot.reverse.forecast(bitcoin.ts, forc, n.ahead=10, title="Bitcoin time series", zoomed.date = 2018.8, fig_num = 40)
```


# 7. MASE

The overall MASE results are therefore as follows: 

```{r}
mase_results
```


# Limitations and reccomendations

**< copied from slack...Need to amend>**

< Meg's words>
My interpretation of limitations would be that our chosen GARCH model does not have the world's greatest residuals. The QQ Plot tails are still fat and there are significant lags in the ACF of the residuals. But maybe we can tie the two together by saying none of the ARIMA GARCH models are perfect and then go into the difficulty of fitting models to financial data more generally

< Sam's words>
Would be that we understand what to expect in regard to the 'sudden outbreaks at irregular intervals with periods of low and high volatility', so this justifies the use of a hybrid ARIMA GARCH model, given that the nonlinear patterns in the residuals of the fitted ARIMA model, being that the GARCH component is equipped for dealing with changes in variance. Comparitively, the GARCH option over the ARCH extension to the ARIMA model is preferable given that the distant volatility in the series is given lesser weight. This may improve upon the fitted model given that for the first half of time series, very little variance was noted compared to the exponential boom and  sudden crash that occurred soon after. This intervention points would still be provided with less weight compared to the changes in variance found in the latter part of the series.

# Conclusion

<Insert conclusion>


# References

Statement of Jennifer Shasky Calvery, Director Financial Crimes Enforcement Network United States Department of the Treasury, Nov 18, 2013

Bambrough, B 2017, Blow To Bitcoin As Mark Zuckerberg Warns Facebook Payments Are Coming, <https://www.forbes.com/sites/billybambrough/2019/03/07/blow-to-bitcoin-as-mark-zuckerberg-warns-facebook-payments-are-coming/#55eaa8566daa>

Bambrough, B 2018, Google Has Suddenly Scrapped Its Bitcoin Ad Ban -- Here's What That Means, <https://www.forbes.com/sites/billybambrough/2018/09/25/google-is-scraping-its-bitcoin-ban-heres-what-that-will-mean/#28aaa60e5654>

Petrica A C, Stancu S, Tindeche A, 2016, Limitation of ARIMA models in financial and monetary economics, Vol 23, No. 4, Winter, Theoretical and Applied Economics, pp. 19-42

Chohan, U W 2017, Assessing the Differences in Bitcoin & Other Cyptocurrency Legality Across National Jurisdictions, UNSW Business School, pp.1-11



# Appendix

< When the project is finished we need to copy the functions and comment what each does >




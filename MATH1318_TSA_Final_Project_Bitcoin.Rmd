---
title: "MATH1318 Final Project: Bitcoin Time Series"
author: "Margaret Cuddihy (s3608125),	Elleni Toumpas (s3708633) and	Samuel Holt (s3381728)"

date: "29 April 2019"
output:
      pdf_document:
      toc: true
      toc_depth: 3
    
    
---

\centering
\raggedright
\newpage
\tableofcontents
\newpage


# Introduction

Bitcoin is a form of electronic cash and is popular for being the first cryptocurrency. It was invented in 2009 and is a decentralized digital currency with transactions recorded by ‘miners’, a process completed through the use of computer processing power (Calvery, 2013). Bitcoin, and cryptocurrency as a whole, is criticised for many factors, its high volatility being one of them. Bitcoin experiences many bubbles and busts, and to be able forecast these trends could help in making decisions before investing or selling bitcoin. Taking into account the daily closing price of bitcoin from the 27th of April 2013 to the 24th of February 2019, sourced from coinmarketcap.com we can build a timeseries model and forecast the next 10 days with high accuracy.

## Outline of Project

The aim of this project is to use the statistical analysis methods covered in MATH1318: Time Series Analysis in order to fit an appropriate model to the Bitcoin time series which will allow us to predict a 10 day forecast. The true values of the next 10 days in the series are known, so the predictive ability of the model will be assessed using the Mean Absolute Scaled Error (MASE) method as will the MASE of the fitted model. 

Firstly, we will import and preprocess the data so it can be converted into a time series object. We will then plot the series and provide an initial inspection of the trend, seasonality, intervention points, change in variance and behaviour of the series. We will also calculate the autocorrelation of the series and examine the Autocorrelation Function (ACF), Partial Autocorrelation Function (PACF) and first lag scatter plot. From this initial inspection, we will highlight key aspects of the series and explain how these will guide our modelling strategy. 

In the next section, we noticed the presence of a trend in the series. This led us to apply a Box Cox transformation and the implementation of differencing of the series to achieve stationarity. The level of differencing will provide the d value of the ARIMA model. We will then model the mean trend of the series through ARIMA modelling, using the ACF, PACF, Extended Autocorrelation Function (EACF) and the Bayesian Information Critereon (BIC) Table to identfy potential orders of p and q parameters. The potential models will be fitted and subjected to statistical testing, whereupon the significance of the coefficients and the behaviour of the model residuals will be assessed. Additionaly, we will use Akaike Information Crieteron (AIC) and BIC scores in order to rank the given models. These methods will be used to filter out irrelevant models and select the best performing models. 

We suspect this series has changing variance. This assumption will be tested and verified using statistical tests and upon validation, a GARCH component will be added to our model. The GARCH orders will be estimated using the transformed residuals of the ARIMA series. The potential GARCH models will be fitted and the residuals will be inspected. Again, we will use AIC to rank the potential models and select the GARCH model with the lowest score. 

Once the ARIMA and GARCH components of our final model have been fitted to the data, we will estimate the 10 day forecast of Bitcoin closing prices. Because we will have transformed and differenced the time series, our forecast will be reverse-transformed and inverse-differenced back to the scale of the original series. We will then assess the accuracy of our prediction using the MASE method.

\newpage

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Packages
```{r message=FALSE, warning=FALSE}
library(stringr)
library(TSA)
library(fUnitRoots)
library(lmtest)
library(tseries)
library(mvtnorm)
library(rugarch)
library(readxl)
```



```{r Appendix, include=FALSE}
acf.pacf.f <- function(x, title1, title2) {
  par(mfrow = c(1,2))
  acf(x, main = title1, cex.main=0.8)
  pacf(x, main = title2, cex.main=0.8)
  par(mfrow = c(1,1))}

plot.compare.f <- function(x1, x2, title1, title2){
  par(mfrow=c(2,1))
  plot(x1, type = "l", main = title1, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  plot(x2, type = "l", main = title2, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  par(mfrow=c(1,1))
}

norm.f <- function(model, titlehist, titleqq)
         {par(mfrow=c(1,2))
            hist(model, main = titlehist, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
            qqnorm(model, main = titleqq, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
            qqline(model, col = 2, lwd = 1, lty = 2)
            print(shapiro.test(model))
            par(mfrow=c(1,1))}

p.q.orders <- function(x, titleA, titleP, ar.max, ma.max, nar, nma)
              {acf.pacf.f(x, titleA, titleP)
              eacf(x, ar.max = ar.max, ma.max = ma.max)
              res = armasubsets(y = x, nar = nar, nma = nma, ar.method = 'yule-walker')
              plot(res)}

arima.test <- function(model, fig_num){
  res <- rstandard(model)
  par(mfrow = c(2,2))
  plot(res, type = 'o', ylab = 'Standardised residuals', main = paste0("Figure ",as.character(fig_num),"A: Time series plot of standardised residuals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  abline(h = 0)
  hist(res, main = paste0("Figure ",as.character(fig_num),"B: Histogram of standardised residuals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  qqnorm(res, main = paste0("Figure ",as.character(fig_num),"C: QQPlot of standardised residuals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  qqline(res, col = 2, lwd = 1, lty = 2)
  acf(res, main = paste0("Figure ",as.character(fig_num),"D: ACF of the Standardised Resiudals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  par(mfrow = c(1,1))
  tsdiag(model, gof = 15, omit.initial = F, main="E:")
  print(shapiro.test(res))
}


# Obtained from the MATH1318 Canvas page for Module 7
sort.score <- function(x, score = c("bic", "aic")){
  if (score == "aic"){
    x[with(x, order(AIC)),]
  } else if (score == "bic") {
    x[with(x, order(BIC)),]
  } else {
    warning('score = "x" only accepts valid arguments ("aic","bic")')
  }
}

arima.garch.residual.test <- function(model, fig_num){
  par(mfrow = c(1,2))
  par(col.main='white') 
  plot(model, which = 8)
  par(col.main='black') 
  title(paste0("Figure ",fig_num,"A: Empirical Density of Standardized Residuals"))
  par(col.main='white') 
  plot(model, which = 9)
  par(col.main='black') 
  title(paste0("Figure ",fig_num,"B: norm - QQ Plot"))
  par(mfrow = c(1,1))
  par(col.main='white') 
  plot(model, which = 10)
  par(col.main='black') 
  title(paste0("Figure ",fig_num,"C: ACF of Standardized Residuals"))
}

# Quick function that orders BIC table
sort.bic <- function(df){
  df <- df[order(df$BIC),]
  df
}

# Capture AIC scores and sort score order
sort.aic.garch <- function(models){
  scores <- as.data.frame(matrix(nrow=length(models), ncol=2))
  colnames(scores) <- c("Model","AIC")
  for (i in 1:length(models)){ 
    scores$Model[i] <- models[i]
    scores$AIC[i] <- infocriteria(get(models[i]))[1] 
  }
  scores <- scores[order(scores$AIC),]
  rownames(scores) <- 1:nrow(scores)
  return(scores)
}

upperbound <- function(value, sigma){
  return(value + 1.96*sqrt(sigma))
}

lowerbound <- function(value, sigma){
  return(value - 1.96*sqrt(sigma))
}

plot.reverse.forecast <- function(t.series, forc, n.ahead, title = NULL, zoomed.date = NULL, fig_num = NULL){
  
  forecasts <- forc@forecast$seriesFor
  f <- frequency(t.series)
  
  # Store all forecast values
  forecast.values <- as.data.frame(matrix(nrow=(n.ahead+1), ncol=9))
  columns <- c('time','values.f', 'values.f.invd', 'values.f.invd.exp','sigma', 'sigma.invd', 'sigma.invd.exp',
             'upper.values.f', 'lower.values.f')
  colnames(forecast.values) <- columns
  
  # Store forecast time
  forecast.values$time <- seq(time(t.series)[length(t.series)]+(1/f), (time(t.series)[length(t.series)]+(1/f))+(n.ahead*1/f), by=(1/f))
  
  # Store forecast values
  forecast.values$values.f <- c(forecasts,NA)
  
  # Store forecast inverse difference values
  forecast.values$values.f.invd <-  diffinv(as.vector(forecasts), xi=log(t.series[length(t.series)]))
  
  # Store forecast inverse exponent values
  forecast.values$values.f.invd.exp <- exp(forecast.values$values.f.invd)
  
  # capture forecasted sigma values
  forecast.values$sigma <- c(as.vector(forc@forecast$sigmaFor),NA)
  
  # inv difference for forecasted sigma values
  forecast.values$sigma.invd <- diffinv(forecast.values$sigma[1:10], xi=log(sd(t.series)))
  
  # exp for forecasted sigma values
  forecast.values$sigma.invd.exp <- exp(forecast.values$sigma.invd)
  
  # Upper and lower values of forecasted values
  forecast.values$upper.values.f <- upperbound(forecast.values$values.f.invd.exp, forecast.values$sigma.invd.exp)
  forecast.values$lower.values.f <- lowerbound(forecast.values$values.f.invd.exp, forecast.values$sigma.invd.exp)
  
  # Turn forecast values into their own timeseries
  forecast.ts <- ts(forecast.values$values.f.invd.exp[1:10], start=(time(t.series)[length(t.series)]+(1/f)),frequency = f)

  # Plot Normal series  
  if (f == 365){
    period = 'day'
  } else if (f == 12) {
    period = 'month'
  } else if (f == 4) {
    period = 'quarter'
  } else {
    period = 'period'
  }
  
  plot(x = t.series, xlim=c(time(t.series)[1],time(t.series)[length(t.series)]+((n.ahead)/f)), ylab="", xlab="Year", main=paste0("Figure ",fig_num," A: ",title, " with ",n.ahead," ",period," forecast"), type="l", cex.main=0.8)
  lines(forecast.ts, col="red", type="l")
  polygon(c((forecast.values$time[1:10]),rev(forecast.values$time[1:10])), c(forecast.values$upper.values.f[1:10],rev(forecast.values$lower.values.f[1:10])), col=rgb(0,0,0.6,0.2), border=FALSE)
  
  if (!is.null(zoomed.date)){
     # Plot Zoomed Series
    plot(x = t.series, xlim=c(zoomed.date,time(t.series)[length(t.series)]+((n.ahead)/365)), ylim=c(3500,4200), ylab="", xlab="Year", main=paste0("Figure ", fig_num," B: ",title," with ",n.ahead," ", period, " forecast ", "(zoomed plot commencing at " , zoomed.date,")"), type="l", cex.main=0.8)
    lines(forecast.ts, col="red", type="l")
    polygon(c((forecast.values$time[1:10]),rev(forecast.values$time[1:10])),
            c(forecast.values$upper.values.f[1:10],rev(forecast.values$lower.values.f[1:10])), col=rgb(0,0,0.6,0.2),
            border=FALSE)
  }
} 

MASE <- function(observed , fitted ){
  # observed: Observed series on the forecast period
  # fitted: Forecast values by your model
  Y.t = observed
  n = length(fitted)
  e.t = Y.t - fitted
  sum = 0 
  for (i in 2:n){
    sum = sum + abs(Y.t[i] - Y.t[i-1] )
  }
  q.t = e.t / (sum/(n-1))
  MASE = data.frame( MASE = mean(abs(q.t)))
  return(list(MASE = MASE))
}

```

# 1. Preprocessing 

Upon inspection of the data, we noticed two things that needed to be addressed. Firstly, the data contains two columns, the first column including dates for each observation. As we planned to convert the data into a time series object, the first column was superfluous and was therefore dropped. Secondly, we found that a few of the daily closing price values were over 1,000 and were recorded with commas. This resulted in NAs when we initially converted the data to a time series object. To prevent this, we simply removed the commas from the data. The time series begins on 27/04/2013 and was given a frequency of 365 as this is a daily series.

## Import Data
```{r}
# setwd("C:/Users/marga/OneDrive/Documents/Uni/Time Series Analysis/Final Project") # Meg's WD
# setwd("C:/Users/samgh/Desktop/Masters of Statistics and Operations Research/Year 2/Sem 1/Time Series Analysis/Assignments/Final Assignment") # Sam's
setwd("~/STUDYING/COURSES/MASTERS-ANALYTICS/03-SEMESTER-01-2019/TIME-SERIES-ANALYSIS/MATH1318_Final_Project") # Elleni's
bitcoin <- read.csv("Bitcoin_Historical_Price.csv", header = TRUE, stringsAsFactors = FALSE)
```

## Inspect the Data
```{r}
dim(bitcoin)
head(bitcoin,2)
bitcoin <- bitcoin[,2] #Only need second column 
```

## Convert to Time Series Object
```{r}
bitcoin <- as.numeric(gsub(",", "", bitcoin)) #Remove commas for thousands and convert to numeric 
which(is.na(bitcoin)) #Check no missing values - none found
bitcoin.ts <- ts(bitcoin, start = c(2013, 4, 27), frequency = 365 )
```

# 2. Intial Inspection of Bitcoin Historical Prices Series

Here we plot and inspect the time series in order to gain an initial understanding of its behaviour. 

## Plot Time Series

```{r, fig.asp = 0.5}
par(mfrow = c(1,1))
plot(bitcoin.ts, ylab = "Daily Closing Price of Bitcoin (USD)", main = "Figure 1: Bitcoin Historical Prices (2013-2019)", cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
```

### Visual Inspection of the Bit Coin Time Series (Figure 1):

1. Trend: There is a exponential increase in the trend until mid-2017 and then it steadily declines for the rest of the series. This will be confirmed using an Augmented Dickey-Fuller Test of stationarity. 

2. Seasonality: There are no seasonally repeating patterns evident in the data. If we miss any seasonal behaviour in the initial inspection, it will be apparently in the ACF and PACF plots. 

3. Changing Variance: Changing variance is very clear. Initially, variance in the closing price value is small with little variance until early 2017 and then it dramatically increases in volitlity. It appears to decrease in variation again at the very end of the series. We know financial data is sensitive to shocks so this is expected (Petrica et al, 2016). Research was conducted to identify the potential causes of these shocks, which included tightening regulation on Bitcoin trading (Chohan, 2017). 

4. Intervention Point: Through the research conducted on the exogenous events impacting the series, we identified a number of potential intervention points, such as the peak and sudden crash in December 2017 and 'big players' banning bitcoin advertisements such as Facebook and Google (Bambrough, 2018) soon after the crash. However, we deemed that none of the shocks were so great as to completely disrupt the behaviour of the series. 

5. Autocorrelation Behaviour: Both autocorrelation and moving average behaviour seem apparent. For autocorrelation behaviour, we can see instances where consecutive observations closely follow each other. For moving average, there are also instances where the data seems to sharply fluctuate. Overall, the behaviour is difficult to see because of the changing variance. 

## Check Autocorrelation between t and t-1

Firstly, we will confirm there is autocorrelation in the series. If there is no significant autocorrelation, it may be sufficient to fit the series using a simple deterministic model. However, given this a financial data time series, this is unlikely to be the case. 

#### Scatter plot of current time point values and their first lag

```{r echo=TRUE, fig.asp = 0.4}
plot(y = bitcoin.ts, x =zlag(bitcoin.ts), ylab='Bitcoin Closing Price (Yt)', xlab='Bitcoin Closing Price (Yt-1)', main = "Figure 2: Scatter plot of current bitcoin series and the previous lag", cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
abline(0,1, col = "red")
```

#### Visual Inspection of Figure 2:

1. As expected, the scatter plot shows there is a clear positive relationship between current time point values and the value of the previous lag.

2. The scatter plot also supports our assumption that there is changing variance in the series. The smaller values of the series are very tightly configured around the 45 degree line, while larger values are more spread.The scatterplot displays clusters present in the autocorrelations of the time series, this is a further indication of changing variance.


#### Calculate the correlation coefficient between current time point values and their first lag

```{r echo=TRUE}
y = bitcoin.ts               
x = zlag(bitcoin.ts)         
index = 2:length(x)    
cor(y[index],x[index])
```

#### Correlation Finding: 

The correlation coefficient calculated is 0.998, indicating extremely strong positive correlation between the current time point value and its first lag. 

Because of the strong autocorrelation, a simple deterministic trend model will not be a suitable fit for this series, as it will be inadequate in capturing the autocorrelation structure. Therefore, we must proceed with an ARIMA model. Further, the initial plot of the time series (Figure 1) and scatter plot of first lags (Figure 2) indicate there is an issue of changing variance so it is likely we will need to include a GARCH component to the final model as well. 

### ACF and PACF of the Time Series

We have established there is autocorrelation in the data. The Autocorrelation Function (ACF) and the Partial Autocorrelation Function (PACF) of the series will provide further insight into the particular behaviour of this autocorrelation. 

```{r, fig.asp = 0.4}
par(mar = c(5, 5, 5, 5)) 
acf.pacf.f(bitcoin.ts,  "Figure 3a: ACF of Bitcoin \n Historical Prices", "Figure 3b: PACF of Bitcoin \n Historical Prices")
#mtext("Small Margins", side = 3, line = 1)
acf(bitcoin.ts, main="Figure 3c: ACF of Bitcoin Historical Prices with max Lags equal to length of series", lag.max = length(bitcoin.ts), cex.main=0.8)
```

#### Inspection of ACF (Figure 3A) and PACF (Figure 3B) of the Bitcoin Historical Prices Time Series:

The ACF in Figure 3A shows a large number of highly significant lags, exhibiting a slowly decaying pattern. The PACF in Figure 3B shows one highly signficant lag at the first lag. The patterns displayed in these plots strongly suggest there is non-stationarity in the data. Non-stationarity is likely to be the cause of the changing trend behaviour identified in the initial inspection of the time series plot (Figure 1). This must be addressed before the data can be modelled.

By referencing Figure 3C we can also see that there is still no evidence of seasonality in our series. In an ACF plot we would see repeating patterns at the beginning of each cycle (at the yearly mark), with strong autocorrelation at these points. As our timeseries doesn't present such patterns we can conclude that there is no seasonality, therefore considering a SARIMA component of the modelling process is not neccessary. 


# 2. Transformation and Differencing

Before fitting the model, any non-stationarity in the series must be reduced. To do this, we will use the Box-Cox method to first transform the data and attempt to stablise the variance. We will then test for stationarity using the Augmented Dickey-Fuller (ADF) Unit-Root Test. Once we have confirmed non-stationarity in the transformed data, we will take the difference of the time series and repeat the ADF test until non-stationarity is achieved. The transformed and differenced data is what we will use in our ARIMA modelling, with the d order equivalent to the number of differences required to achieve stationarity. 

## Apply a Box-Cox Transformation 

Due to the ACF and PACF correlograms demonstrating non-stationarity of the time series data, we will investigate the possible and optimal values of lambda using a Box Cox transformation.
 
```{r, fig.asp = 0.4}
bitcoin.bc <- BoxCox.ar(bitcoin.ts, method = 'yule-walker')
bitcoin.bc$ci
```

The optimal lambda is 0, indicating a log transformation on the bitcoin time series is most appropriate. Since all values of the time series exceed 0, no arbitrary scalar value will have to be added to the series for transformation.

```{r}
bitcoin.log <- log(bitcoin.ts)
plot.compare.f(bitcoin.ts, bitcoin.log, "Figure 4a: Original Bitcoin Historical Prices (2013-2019)", "Figure 4b: Log-Transformed Bitcoin Historical Prices (2013-2019)" )
```

### Visual Inspection of the Log Transformed Series Plot

The log transformation of the series has removed some of the drastic change in variance that occurs around 2017, though there is still some  volatility clustering apparent. The plot in Figure 4b now clearly shows an upward trend apparent in the data, further supporting our expectation of non-stationarity in the data.

### Check whether the log transformation has improved the normality of the series.

```{r, fig.asp = 0.4}
norm.f(bitcoin.ts, "Figure 5a: Historgram of Log Bitcoin Prices", "Figure 5b: QQ Plot of Log Bitcoin Prices")
norm.f(bitcoin.log, "Figure 5c: Historgram of Log Bitcoin Prices", "Figure 5d: QQ Plot of Log Bitcoin Prices")
```

### Result of Normality Tests for Log-Transformed Series

The normality of the distribution has considerably improved. The histogram of the transformed data (Figure 5c) is far less postiviely skewed than the original data (Figure 5a) so there has been some improvement to the distribution. The QQ Plot (Figure 5c) shows that the data now more closely resembles the red reference line of normality, though there is still signficant deviation, particularly around the tails. The thickness of the tails in the QQ Plot is an indication of changing variance. Further, the Shapiro-Wilk test result supports that the data is not normally distributed, even after the log transformation.

### ACF and PACF of the Log-Transformed Series

```{r, fig.asp = 0.4}
acf.pacf.f(log(bitcoin.ts), "Figure 6a: ACF of \n Log Bitcoin Prices", "Figure 6b: PACF of \n Log Bitcoin Prices")
```

### Visual Inspection of the Log Transformed Series ACF and PACF

Although the distribution of the data was somewhat improved by the log transformation, the ACF (Figure 6a) and PACF (Figure 6b) of the log-transformed series still strongly indicate that the series is non-stationary. An upward trend was also obvious in the plot of the transformed data (Figure 5a) This will be confirmed using a unit-root test. If confirmed, differencing of the series will be used to render the series stationary. 

## Augmented Dickey-Fuller Unit-Root Test of Trend Stationarity

The null hypothesis of the test is that the data is trend non-stationary; that is, the data is not stationary but can be rendered so through differencing. The alternative hypothesis is that the data is stationary. We will use the significance level of 95%, so a p-value less than 0.05 will result in a rejection of the null hypothesis. 

```{r}
ar(diff(bitcoin.log)) #Order of lags select = 31
adfTest(bitcoin.log, lags = 31, type = "nc", title = NULL,description = NULL)
```

### Results of the ADF Test

The p-value of the ADF unit-root test is much higher than 0.05, leading the the failure to reject the null hypothesis of trend non-stationarity. We can proceed with differencing the series to remove the trend. 

## First Differencing 

```{r, fig.asp = 0.8}
bitcoin.r <- diff(bitcoin.log)
plot.compare.f(bitcoin.log, bitcoin.r, "Figure 7a: Log-Transformed Bitcoin Historical Prices (2013-2019)", "Figure 7b: First Difference of Log-Transformed Bitcoin Historical Prices (2013-2019)")
```

### Results of First Differencing - Plot Inspection

Taking the first difference of the log-transformed Bitcoin Historical Prices series has appeared to largely remove the positive trend from the series. Now the data appears to fluctuate around a mean of zero. The volatility clustering is clearly evident, with large variance clusters occuring around midway through 2013 and 2017 in particular. This indicates that a GARCH component may need to be included in our final model as forecasting will need to take into account conditional variance. 

## Augmented Dickey-Fuller Unit-Root Test of Trend Stationarity for the First Difference

Stationarity will be confirmed with an ADF unit-root test applied to the first difference of the log-transformed series. 

```{r}
ar(diff(bitcoin.r)) #Order of lags select = 32
adfTest(bitcoin.r, lags = 32, type = "nc", title = NULL,description = NULL)
```

The p-value of the ADF test when applied to the first difference of the log-transformed series is less than 0.01. With a p-value less than the given alpha level we then reject the null hypothesis and conclude that the series has be rendered stationary. 

This combination of taking the log-transformation and first difference of the series is also commonly referred to as the returns of the series. The returns indicate the percentage movement of prices for each day. We will proceed with determining the ARIMA orders of the Bitcoin Historical Prices series using the returns.

# 3. Determining the Potential Orders of an ARIMA Model

An ARIMA(p,d,q) model will be fitted and used to predict future mean levels of Bitcoin prices. In Section 2 of the report, it was determined that the differencing order will be 1 as taking the first difference successfully rendered the time series stationary (d = 1). The potential orders of p and q will be estimated using the ACF, PACF, Extended ACF (EACF) and Bayes Information Critereon (BIC) Table of the returns series of Bitcoin Historical Prices. 

`
```{r, fig.asp = 0.4}
p.q.orders(bitcoin.r, "Figure 8a: ACF of Returns", "Figure 8b: PACF of Returns", 10, 10, 13, 12)
```


### ACF and PACF Estimated Orders of p and q

Taking the first difference of the Bitcoin prices series has removed the slow decaying pattern of highly signficant lags in the ACF (Figure 8a). The ACF and PACF (Figure 8b) show a number of signficant lags, but due to the changing variance in the series, the interpretation of the likely orders of p and q is not clear. For the ACF, there are definitely 2 signficant lags, possibly 4 or even 6 that could be considered candidate values for q. Similarly in the PACF, there are definitely two signficant lags towards the start and possibly 4 or 6 signficant lags. The changing variance is also likely to be the cause of higher order lags further back in the series which are less clearly significant. 

Possible ARIMA models: {ARIMA(2,1,2), ARIMA(2,1,4), ARIMA(2,1,6), ARIMA(4,1,2), ARIMA(4,1,4), ARIMA(4,1,6), ARIMA(6,1,2), ARIMA(6,1,4), ARIMA(6,1,6)}

### EACF Estimated Orders of p and q

On initial inspection of the EACF plot, it appears the highest left vertex is at (0,0) indicating that the returns series is a white noise series, but because there has been evidence of changing variance in the series, it is likely that the EACF output is "fuzzy"; that is, the location of the vertex is not clear. Another possible vertex point exists at (5,5), (6,5), (5,6) and (6,6) so we will take these as possible orders. 

Possible ARIMA models: {ARIMA(5,1,5), ARIMA(5,1,6), ARIMA(6,1,5), ARIMA(6,1,6)}

### BIC Table Estimated Orders of p and q

The BIC table indicates the models with the low BIC values tend to have AR orders of 6 and 11 and MA order of 11.  The BIC Table tends to yield quite high orders, but since the series has over 2000 observations these orders are not unreasonable.  

Possible ARIMA models: {ARIMA(6,1,0), ARIMA(6,1,11), ARIMA(11,1,11)}


# 4. Testing Potential ARIMA Models

The final set of models to be tested is:

{ARIMA(2,1,2), ARIMA(2,1,4), ARIMA(2,1,6), ARIMA(4,1,2), ARIMA(4,1,4), ARIMA(4,1,6), ARIMA(5,1,5), ARIMA(5,1,6), ARIMA(6,1,0), ARIMA(6,1,2), ARIMA(6,1,4), ARIMA (6,1,5) ARIMA(6,1,6), ARIMA(6,1,11), ARIMA(11,1,11)}

The methodology for chosing the best ARIMA model will firstly involve fitting each set of ARIMA orders to the data using the Maximum Likelihood Estimation (MLE) method and inspecting the coefficients generated by the model to see if they are shown to be statistically significant. Models with all or most coefficients significant will be prioritised. We will also inspect the residuals of each model by inspecting the plot of the standardised residuals, the ACF of the standardised residuals and by calculating the p-values for the Ljung-Box statstic at each lag. In the ACF of the residuals, we will be looking to see if the model has sufficiently captured the autocorrelation of the series, leaving only white noise. Models generating ACFs with no significant lags will be prioritised. For the Ljung-Box test, the null hypothesis is that there is no autocorrelation left in any part of the residuals. Therefore we will be looking for p-values greater than the significance threshold of 0.05 to indicate that the model has suffcienctly captured autocorrelation. Models will p-values higher than 0.05 at all lags will be prioritised. 


**ARIMA(2,1,2)**

In our first ARIMA test we explore the candidate ARIMA(2,1,2) model. In the coefficient test we can see that we have two coefficients that are statistically insignificant (with a p-value greater that the significance level of 0.05) and two values with NAs in the Standard Error and p-values of the coefficients.Insignificant p-values allude to the fact that this candidate model does not capture the series well.  The NA values are caused by the square root of negative values found in the hessian matrix which is passed when fitting an ARIMA model.

In the ACF plot of Standardised Residuals (figure 9D) we still have significant ACF lags, corresponding to some autocorrelation left in the residuals. In the Ljung Box test (titled p-values for Ljung Box statistic) we have a mix of  statistically signficant and statistically insignificant lags. This deems the model as not valid.


```{r}
b.212 <- arima(bitcoin.log, order=c(2,1,2), method = 'ML')
coeftest(b.212)
arima.test(b.212, 9)
```

**ARIMA(2,1,4)**

The next canidate model, ARIMA(2,1,4), is tested below. In the coefficient test we can see only two statistically significant MA coefficients ($\theta_2$ and $\theta_4$) with the rest of the coefficient estimates being statistically insignificant. 

In the ACF plot of Standardised Residuals (figure 10D) we again still have significant ACF lags with the Ljung Box test (titled p-values for Ljung Box statistic) having mainly statistically insignificant lags. But due to the significant lags at 11 onward, this model is also not viable.

```{r}
b.214 <- arima(bitcoin.log, order=c(2,1,4), method = 'ML')
coeftest(b.214)
arima.test(b.214, 10)
```

**ARIMA(2,1,6)**

With the candidate model ARIMA(2,1,6) half the coefficients are statistically significant ($\phi_1$, $\phi_2$, $\theta_1$ and $\theta_2$). This could be a symptom of overfitting, which will be investigated if this model proves to be significant.

In the ACF plot of Standardised Residuals (figure 11D) we only have late significant ACF lags.

The Ljung-Box test (titled p-values for Ljung Box statistic) we have no lags that fall below the significant level, meaning that there is no significant autocorrelation left in the lags of the residuals of this candidate model. We then deem ARIMA(2,1,6) as a candidate model.

```{r}
b.216 <- arima(bitcoin.log, order=c(2,1,6), method = 'ML')
coeftest(b.216)
arima.test(b.216, 11)
```

**ARIMA(4,1,2)**

In the candidate model ARIMA(4,1,2) we have only one statistically significant coefficient ($\phi_4$). We can see late lags in the ACF plot of residuals (plot 12D) but we also see that the Ljung-Box test on the candidate model's residuals show some lags that are statistically significant. We interpret this to mean that there is some autocorrelation left in these lags and thus reject the validity of this model.

```{r}
b.412 <- arima(bitcoin.log, order=c(4,1,2), method = 'ML')
coeftest(b.412)
arima.test(b.412, 12)
```

**ARIMA(4,1,4)**

In the candidate model ARIMA(4,1,4) we have the first candidate model with all coefficients returning statistically significant. 

In the plot of the standardised residuals (Figure 13A) we see points hovering over a mean value of 0, however they are not independantly distributed. We still see changes in variation across the residual plot. 

In the ACF plot of Standardised Residuals (figure 13D) we only have late significant ACF lags. The Ljung-Box test (titled p-values for Ljung Box statistic) also has no lags that fall below the significant level, meaning that there is no significant autocorrelation left.

This candidate model seems promising in consideration for the capturing the ARIMA component of the Bitcoin series.

```{r}
b.414 <- arima(bitcoin.log, order=c(4,1,4), method = 'ML')
coeftest(b.414)
arima.test(b.414, 13)
```



**ARIMA(4,1,6)**

In the candidate model ARIMA(4,1,6) we only three statistically significant coefficients ($\phi_4$, $\theta_4$ and $\theta_6$)

In the ACF plot of Standardised Residuals (figure 14D) we only have late significant ACF lags with the Ljung Box test (titled p-values for Ljung Box statistic) having statistically significant lags. 

The standardised residuals display change in variance over the time series, this can be addressed using a GARCH component if the model shows no significant lags in the Ljung Box test. As displayed below, there are indeed no significant lags, deeming ARIMA(4,1,6) a candidate model.

```{r}
b.416 <- arima(bitcoin.log, order=c(4,1,6), method = 'ML')
coeftest(b.416)
arima.test(b.416, 14)
```

**ARIMA(5,1,5)** 

In the next candidate model ARIMA(5,1,5) we have a mix of results in the coefficient test. Two coefficients ($\phi_4$ and $\theta_4$) produce NA values in the test for statistical significance. One value ($\theta_3$) is statistically insignificant, leaving the rest of the coefficients statistically significant. 

In the ACF plot of Standardised Residuals (figure 15D) we yet again only have late significant ACF lags with the Ljung Box test (titled p-values for Ljung Box statistic) having statistically significant lags.

Again, change in variance is present in the standardised residual plot, deeming the possible use of a GARCH component. The Ljung Box test showsno significant lags, deeming ARIMA (5,1,5) as a candidate model.

```{r}
b.515 <- arima(bitcoin.log, order=c(5,1,5), method = 'ML')
coeftest(b.515)
arima.test(b.515, 15)
```


**ARIMA(5,1,6)**

Testing the hypothesised ARIMA(5,1,6) model below we find four coefficients statistically insignificant  ($\phi_4$ and $\theta_1$, $\theta_4$ and $\theta_6$). The rest of the coefficients are statistically significant.

The histogram of residuals (in Figure 16B) shows the residuals are symmetrically distributed but the shape of the bell curve is very extreme.  The QQ Plot (Figure 16C) shows very thick tails indicating changing variance which will need to be handled. The Shapiro-Wilk result is a very small p-value, indicating the rejection of the null hypothesis of normally distributed residuals. 

The ACF of the Standardised Residuals (Figure 16D) indicates no autocorrelation left in the residuals. The p-value of the Ljung-Box test is greater than 0.05, indicating that there is no signficant evidence to reject the null hypothesis that the residuals are uncorrelated. 

From the plot (Figure 16A), there is still obvious volatility clustering that can be handled by proceeding with a GARCH modelling component.

As a result we have found this candidate model a viable model for further exploration.

```{r}
b.516 <- arima(bitcoin.log, order=c(5,1,6), method = 'ML')
coeftest(b.516)
arima.test(b.516, 16)
```

**ARIMA(6,1,0)**

Testing the candidate model ARIMA(6,1,0) we find that all coefficients are statistically insignificant, except for the coefficients ($\phi_5$ and $\phi_6$). This may indicate not overfitting, but simply that most coefficients of lower orders are not useful for capturing all stochastic processes of the time series. 

Histogram of residuals (Figure 17B) shows they are symmetrically distributed but the shape of the bell curve is very extreme. The QQ Plot (Figure 17C) shows very thick tails indicating changing variance which will need to be handled.  Shapiro-Wilk result is a very small p-value, indicating the rejection of the null hypothesis of normally distributed residuals. 

The ACF of Residuals (Figure 17D) indicates there is autocorrelation left in the residuals. However the p-value of the Ljung-Box test supports the argument that the lags are not statistically significant. This allows us to deem ARIMA (6,1,0) as a candidate model.


```{r}
b.610 <- arima(bitcoin.log, order=c(6,1,0), method = 'ML')
coeftest(b.610)
arima.test(b.610, 17)
```

**ARIMA(6,1,2)**

When testing the ARIMA(6,1,2) candidate model we can see insignificant coefficients on all estimates from $\phi_3$ t0 $\phi_6$. The higher orders of $\phi$ may be an indication of overfitting. 

In the ACF plot of the Residuals (Figure 18D) we can see late autocorrelated lags. Thick tails appear in the QQ Plot (Figure 18C) and with volatility clustering in the plot of residuals (Figure 18A). The Ljung-Box test shows that all lags are statistically insignificant. This deems ARIMA(6,1,2) as a valid candidate model.


```{r}
b.612 <- arima(bitcoin.log, order=c(6,1,2), method = 'ML')
coeftest(b.612)
arima.test(b.612, 18)
```

**ARIMA(6,1,4)**

When testing the ARIMA(6,1,4) candidate model we can see insignificant coefficients in two estimates ($\phi_5$ and $\phi_6$) with $\phi_1$ and $\phi_4$ as statistically significant coefficients and coefficients with NA p-values over the rest. This implies the infinite approach to a given value or the square root of a negative value and possible overfitting with insignificant higher order values.

In the ACF plot of the Residuals (Figure 19D) we can see late autocorrelated lags, with yet again thick tails appearing in the QQ Plot (Figure 19C) and volatility clustering in the plot of residuals (Figure 19A). The Ljung-Box test shows that all lags are statistically insignificant. This allows us to deem ARIMA(6,1,4) as a valid candidate model.


```{r}
b.614 <- arima(bitcoin.log, order=c(6,1,4), method = 'ML')
coeftest(b.614)
arima.test(b.614, 19)
```


**ARIMA(6,1,5)**

In the ARIMA(6,1,5) candidate model, most coefficient estimates are signficant except for $\phi_4$, $\phi_6$ and  $\theta_4$.

In the ACF plot of the Residuals (Figure 20D) we can see only one autocorrelated lag and yet again thick tails appear in the QQ Plot (Figure 20C) with volatility clustering in the plot of residuals (Figure 20A). 

The Ljung-Box test shows that all lags are statistically insignificant, meaning the lags hold no statistically significant autocorrelaton. This allows us to deem ARIMA(6,1,5) as a valid candidate model.

```{r}
b.615 <- arima(bitcoin.log, order=c(6,1,5), method = 'ML')
coeftest(b.615)
arima.test(b.615, 20)
```

**ARIMA(6,1,6)**

In the ARIMA(6,1,6) candidate model, all coefficient estimates return a NA p-value. This indicates that upon the passing of the hessian matrix within the ARIMA model fit, negative values were involved and thus we may consider rejecting this model. We shall investigate further the residuals and lags to determine the validity of this model.

In the ACF plot of the Residuals (Figure 21D) we can see only one, maybe two autocorrelated lag and the Ljung-Box test shows that all lags are statistically insignificant, meaning the lags hold no statistically significant autocorrelaton.

Thick tails appear in the QQ Plot (Figure 21C) with volatility clustering in the plot of residuals (Figure 21A). This aspect not being captured by the model could be handled by a GARCH component, considered a later in the report.

The Ljung Box test shows no significant lags thus we deem ARIMA(6,1,6) as a statistically valid model to consider.

```{r}
b.616 <- arima(bitcoin.log, order=c(6,1,6), method = 'ML')
coeftest(b.616)
arima.test(b.616, 21)
```

**ARIMA(6,1,11)**

In the ARIMA(6,1,11) we have only six statistically significant coefficient estimates, with three coefficients with NA p-values and the rest all statistically insignificant. Although the highest order of $\phi$ is significant, all higher orders of $\theta_7$ onward do not show any statistical significance. This could indicate overfitting.

We have two lags autocorrelated found in the ACF plot of residuals in Figure 22D. The Ljung-Box test shows that all lags are statistically insignificant, meaning the lags hold no statistically significant autocorrelaton.

Thick tails appear in the QQ Plot (Figure 21C) with volatility clustering in the plot of residuals (Figure 21A). This aspect not being captured by the model could be handled by a GARCH component, considered a later in the report.

The Ljung Box test shows no significant lags thus we deem ARIMA(6,1,11) as a statistically valid model to consider.


```{r}
b.6111 <- arima(bitcoin.log, order=c(6,1,11), method = 'ML')
coeftest(b.6111)
arima.test(b.6111, 22)
```

**ARIMA(11,1,11)**

In the test on ARIMA(11,1,11) we find nine coefficient estimates to be statistically signficant and the rest are statistically insignificant. Given that the highest orders of $\phi$ and $\theta$ are significant, the insignificant variables do not indicate overfitting.

In the ACF plot of the Residuals (Figure 23D) we can see only one late significant lag with the Ljung-Box test showing that all lags are statistically insignificant, meaning the lags actually hold no statistically significant autocorrelaton.

Thick tails appear in the QQ Plot (Figure 21C) with volatility clustering in the plot of residuals (Figure 21A). This aspect not being captured by the model could be handled by a GARCH component, considered a later in the report.

The Ljung Box test shows no significant lags thus we deem ARIMA(11,1,11) as a statistically valid model to consider.

```{r}
b.11111 <- arima(bitcoin.log, order=c(11,1,11), method = 'ML')
coeftest(b.11111)
arima.test(b.11111, 23)
```

Based on this analysis, we can narrow down our selection of possible models. The models that successfully removed autocorrelation from the series were {ARIMA(2,1,6), ARIMA(4,1,6), ARIMA(5,1,5), ARIMA(5,1,6), ARIMA(6,1,0), ARIMA(6,1,4), ARIMA(6,1,5), ARIMA(6, 1, 11), ARIMA(11,1,11)}.


### AIC BIC Ranking

The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) score will be calculated for each model. The models will be ranked in descending order and the model with the lowest score will be considered further in the ARIMA analysis.

```{r}
sort.score(AIC(b.216,b.416,b.515,b.516, b.610,b.612, b.614, b.615, b.6111, b.11111), score = "aic")
sort.bic(BIC(b.216,b.416,b.515,b.516, b.610,b.612, b.614, b.615, b.6111, b.11111))
```

### Selecting the best Model

In selecting the best model from the ARIMA modelling process, we need to balance the results from all the residual tests, coefficient estimates (and their statistical significance) and AIC/ BIC scores. Collating all the observations in the below table, we narrow down the best model with the following method:

1. Firstly we drop any model that returns significant lags in the Ljung Box test or models that return many autocorrelated lags in the ACF plot of residuals. If a model fails either of these tests, we can presume that the candidate model doesn't capture the autoregressive behaviour present in the model, which is the main motivation of the ARIMA process. 

2. The second phase occurrs with the remaining models. Taking into consideration the AIC and BIC scores, we sort the models from lowest to highest score. We then continue the process, taking into consideration the candidate models that rank in the first and second positions for both AIC and BIC.

3. Finally we settle on the best model that also shows the most statistically significant coefficients, incidicating that the proposed model and its coefficients efficiently captures the behaviour of the series.

```{r include=FALSE}

collated.results <- as.data.frame(matrix(ncol = 7, nrow = 0), stringAsFactor = FALSE)
colnames(collated.results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')

results <- list('b.212','1/2 insignificant 1/2 NA','3/4 significant','many early lags','-','2','drop')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.214','ma2 and ma4 significant','1/2 significant or close to alpha', '3/4 lags', '-', '-', 'drop')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.216','all ar significant, all ma insignificant','all insignificant','3 lags, late in plot','8','-','')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.412','all insignificant except AR4','1/2 less than alpha or low','lots of lags from 1/4 in','-','-','drop')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.414', 'all significant', 'all insignificant', '2-3 lags midway through','-','-','-')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.416','ar4, ma4, ma6 significant, rest insignificant','all insignificant','5 - 6 lags from 1/3 in', '10', '8', '' )
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.515', '2 NA, 7 significant, 1 insignificant','all insignificant', '2-3 lags, 2 others close to alpha','4', '6', '')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.516','7 significant, 3 just above alpha, 1 insignificant','all insignificant', '1 lag, 2 just under', '1', '4', 'TOP AIC')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.610','all but 1 insignificant','all insignificant', '7 lags from 1/3 in','9','3','')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.612', 'ar1, ar2, ma1, ma2 significant, rest insignificant', 'all insignificant', '3 lags midway', '7', '1', 'TOP BIC')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.614', '4 NAs, 4 significant, 2 insignificant', 'all insignificant', '3 lags midway', '5', '7', '')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.615', '8 significant, 3 insignificant', 'all insignificant', '1 lag mid, 2 close to alpha', '2', '5', '2ND AIC')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.616', 'all NAs', 'all insignificant', '2 lags (just)', '-', '-', '')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.6111','6 significant, 2 NA', 'all insignificant', '2 lags', '6', '9', '')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

results <- list('b.11111', '9 significant , 4 NA', 'all insignificant', '1 late lag', '3', '10', '')
names(results) <- c('model','coeff.pvalues','ljungbox','acf','aic.order','bic.order','result')
collated.results <- rbind(collated.results, results, stringsAsFactors=FALSE)

```

```{r}
collated.results
```

### Final ARIMA Model Chosen:

In the collated results above we can see that b.212, b.214 and b.412 fail the Ljung Box test, with many significant lags. This indicates the residuals display serial autocorrelation, meaning the model has failed to adquately capture the auotcorrelation behaviour of the series. 

By ordering the remaining AIC and BIC scores we see that the top 2 AIC scores are for the models b.516 and b.615. The top 2 performing BIC models are b.612 and b.212. b.212 is a model we have rejected in the previous phase because of serial autocorrelation, therefore the models we consider in the last step are: b.516, b.615 and b.612.

In the final phase we look at the coefficient estimates and the p-values for b.516, b.615 and b.612. Out of these three models b.516 has the best performing coefficient estimates with 7 statistically significant coefficients, and with 4 statistically insignificant coefficients. Out of the 4 insignificant coefficients 3 are only just above the statisticaly insignificance alpha. 

From the standardised residuals plots of all the models, there is still obvious volatility clustering that will need to be examined further.


### Overfitting Testing

Before we accept ARIMA(5,1,6) as the chosen model, we will use overfitting; that is, we will confirm that the next-largest model does not perform better. Since ARIMA(6,1,6) was one of the candidate models, only ARIMA(5,1,7) will be tested. 

```{r}
b.517 <- arima(bitcoin.log, order=c(5,1,7), method = 'ML') 
coeftest(b.517)
AIC(b.516, b.517)
arima.test(b.517, 24)
```

Although the models seems to have generated residuals free of autocorrelation, it also generated a higher AIC value than ARIMA(5,1,6). With the added order $\theta_7$ also producing a statistically insignificant estimate, we can safely conclude that there is no compelling evidence that the ARIMA(5,1,7) model performs better that the chosen model ARIMA(5,1,6).




# 5. Modelling Conditional Variance with GARCH

We can use ARIMA(5,1,6) to model and forecast future mean levels of Bitcoin prices. However, throughout the report so far there has been strong evidence of conditional variance (i.e. variance that changes over time). In the original time series plot (Figure 1), there was evidence of changing variance over time. Volatility clustering was even clearer in the plot of the log-transformed first difference of the series (Figure 7b). After our chosen ARIMA model was fitted, there was still volatility clustering present in the standardised residuals plot and in the the thick tails of the QQ Plot.

In order to incorporate this conditional variance into the Bitcoin Historical Prices series modelling, we will examine two transformations of the returns residuals and estimate orders p and q of GARCH to combine with ARIMA(5,1,6) so that we can model both mean and variance of the series. 

## Confirming the Presence of ARCH

### Plot of Standardised Residuals

```{r}
bitcoin.res <- rstandard(b.516)
plot.compare.f(bitcoin.r, bitcoin.res, "Figure 25a: Plot of Returns Series ", "Figure 25b: Plot of ARIMA(5,1,6) Standardised Residuals")
```

In the plot of the residuals of the ARIMA(5,1,6) model, volatility clustering is still apparent.

### ACF and PACF of Standardised Residuals

```{r}
acf.pacf.f(bitcoin.res, "ACF", "PACF")
eacf(bitcoin.res)
```

The ACF, PACF and EACF of the residuals all indicate white noise. This can imply that the ARIMA model has perfectly captured autocorrelation but it can also indicate changing variance. 

### Normality Test of Standardised Residuals 

If the ARIMA(5,1,6) model had perfectly captured all the autocorrelation structure of the series, we would end up with i.i.d. variables in the residuals. However, looking at the QQ plot, (Figure 16C in the ARIMA modelling process), it is clear the residuals do not follow the line of normal distribution. The fat tails on either end are an indicator of conditional variance in the series. 

### McLeod-Li Test of Standardised Residuals 

```{r}
McLeod.Li.test(y = bitcoin.res, main = " Figure 26: McLeod-Li Test Statistics", gof.lag = 30, cex.main=0.8)
```

The null hypothesis of the McLeod-Li test (Figure 26) is there is no Autoregressive Conditional Heteroskedasticity (ARCH) in the series. All p-values of the McLeod-Li test are statistically significant, indicating that we reject the null hypothesis and conclude there is ARCH behaviour in this series. 

### ACF and PACF of Nonlinear Transformations

If the residual values are truly independent, then applying nonlinear transformations to the data should not affect the autocorrelation structure of the data. We should still see no signficant lags in the ACF and PACF as is the case for the untransformed residuals (Yaziz et al, 2013). 

```{r}
# Absolute Value Transformtaion
acf.pacf.f(abs(bitcoin.res), "Figure 27a: ACF of Abs. \n Transformed Standardised Residuals", "Figure 27b: PACF of Abs.\n Transformed \n Standardised Residuals")

# Squared Value Transformtaion
acf.pacf.f(bitcoin.res^2, "Figure 27c: ACF of Sqr. \n Transformed Standardised Residuals", "Figure 27d: PACF of Sqr.\n Transformed \n Standardised Residuals")
```

Comparing Figures 27a and 27b of the Absolute Values transformed residuals with Figures 27c and 27d of the Squared Values transformed residuals, the ACF and PACF of the different transformations are very different. Further, there are a large number of signficant lags. Therefore the assumption of i.i.d. residuals is violated. 

This is sufficient evidence to conclude there is an ARCH behaviour in the variance of the series. We can estimate a GARCH component using the EACF of the transformed residuals to improve the accuracy of our ARIMA(5,1,6) model.

## Estimate the Orders of GARCH 

Given the clear indication of changing variance and conditional heteroscedasticity from previous plots of standardised residuals, a GARCH element will be investigated. For finanical data that is volatile as bitcoin, the use of the GARCH extension, as opposed to the ARCH option, on the ARIMA model 'improves the accuracy of forecasting by including all the past squared returns with lesser weights corresponding to more distant volatilities' (Grachev, 2017), thus regarding the changes in volitility in the bitcoin data, this allows us to confidently implement a GARCH component.

\newpage

### EACF of Transformed Residuals

```{r}
eacf(abs(bitcoin.res))
```

For the Absolute Values Transformed Residuals, the vertex appears to fall at (2,2).

Possible values of p: 2, 3. Possible values of q: 2, 3. Possible values of Max(p,q): 2,3

Possible Models: {GARCH(2,2) GARCH(3,2), GARCH(3,3)}


```{r}
eacf((bitcoin.res)^2)
```

The vertex is a bit less clear for the Squared Values Transformed Residuals, but possibly falls at (4,4) or (4,5).

Possible values of p: 4, 5. Possible values of q: 4, 5, 6. Possible values of Max(p,q): 4, 5, 6

Possible Models: {GARCH(4,4), GARCH(5,4). GARCH(5,5), GARCH(6,4), GARCH(6,5)}


## Fit Parameter Estimates

We will fit both the ARMIA(5,1,6) and the possible GARCH orders to the data. We will look for the model with the lowest AIC score. Ideally, we will see mostly significant coefficients and some improvement to the residuals as well. 

### Absolute Value Transformation

**GARCH(2,2)**

In the possible GARCH(2,2) model we can see only 1 lag in the ACF of standardized residuals (Figure 28C). The QQ Plot (Figure 28B), yet again, presents thick tails.

In the coefficient estimates, 2 coefficients ($\theta_6$ and $\alpha_2$) are statistically insignificant.


```{r, fig.asp = 0.4}
a.22 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(2, 2)), mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")
m.56_22 <- ugarchfit(spec = a.22, data = bitcoin.r, out.sample = 100)
#m.56_22
m.56_22@fit$matcoef  
infocriteria(m.56_22)
arima.garch.residual.test(m.56_22, 28)
```

**GARCH(3,2)**

In the possible GARCH(3,2) model we can see only 3 lags in the ACF of standardized residuals (Figure 29C). The QQ Plot (Figure 29B), yet again, presents thick tails. 

In the coefficient estimates, 3 coefficients ($\theta_5$, $\alpha_2$ and $\alpha_3$) are statistically insignificant.

```{r, fig.asp = 0.4}
a.32 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(3, 2)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")

m.56_32 <- ugarchfit(spec = a.32, data = bitcoin.r, out.sample = 100)
# m.56_32  
m.56_32@fit$matcoef  
infocriteria(m.56_32)
arima.garch.residual.test(m.56_32, 29)
```

**GARCH(3,3)**

In the possible GARCH(3,3) model we can have only 1 lag in the ACF of standardized residuals (Figure 30C). The QQ Plot (Figure 30B), yet again, presents thick tails. 

In the coefficient estimates, 5 coefficients ($theta_6$, $\alpha_2$, $\alpha_3$, $\beta_1$, $\beta_3$) are statistically insignificant.

```{r, fig.asp = 0.4}
a.33 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(3, 3)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE), distribution.model = "norm")

m.56_33 <- ugarchfit(spec = a.33, data = bitcoin.r, out.sample = 100)
m.56_33@fit$matcoef  
infocriteria(m.56_33)
arima.garch.residual.test(m.56_33, 30)
```


### Square Root Transformation

**GARCH(4,4)**

In the possible GARCH(4,4) model we can have only 1 lag in the ACF of standardized residuals (Figure 31C). The QQ Plot (Figure 31B), yet again, presents thick tails. 

In the coefficient estimates, 7 coefficients ($\theta_1$, $\theta_6$, $\alpha_2$, $\alpha_3$, $\alpha_4$, $\beta_2$, and $\beta_3$) are statistically insignificant.

```{r, fig.asp = 0.4}
a.44 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(4, 4)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")
m.56_44 <- ugarchfit(spec = a.44, data = bitcoin.r, out.sample = 100)
m.56_44@fit$matcoef  
infocriteria(m.56_44)
arima.garch.residual.test(m.56_44, 31)
```

**GARCH(5,4)**

In the possible GARCH(5,4) model we can have only 1 lag in the ACF of standardized residuals (Figure 32C). The QQ Plot (Figure 32B), yet again, presents thick tails. 

  
In the coefficient estimates, 7 coefficients ($\theta_6$, $\alpha_2$, $\alpha_3$, $\alpha_4$, $\alpha_5$, $\beta_2$, and $\beta_3$) are statistically insignificant.

```{r, fig.asp = 0.4}
a.54 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(5, 4)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")
m.56_54 <- ugarchfit(spec = a.54, data = bitcoin.r, out.sample = 100)
m.56_54@fit$matcoef  
infocriteria(m.56_54)
arima.garch.residual.test(m.56_54, 32)
```

**GARCH(5,5)**

In the possible GARCH(5,5) model we can have only 1 lag in the ACF of standardized residuals (Figure 35C). In Figure 35B the QQ Plot, yet again, presents thick tails. 

In the coefficient estimates, 6 coefficients ($\theta_6$, $\alpha_4$, $\beta_1$, $\beta_2$, $\beta_3$, and $\beta_4$) are statistically insignificant.

```{r, fig.asp = 0.4}
a.55 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(5, 5)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")
m.56_55 <- ugarchfit(spec = a.55, data = bitcoin.r, out.sample = 100)
m.56_55@fit$matcoef  
infocriteria(m.56_55) 
arima.garch.residual.test(m.56_55, 33)
```

**GARCH(6,5)**

In the possible GARCH(6,5) model we can have only 2 lags in the ACF of standardized residuals (Figure 34C). In Figure 34B the QQ Plot, yet again, presents thick tails. 

In the coefficient estimates, 13 coefficients ($\phi_1$, $\phi_4$, $\theta_1$, $\theta_4$, $\theta_6$, $\alpha_2$, $\alpha_4$, $\alpha_5$, $\alpha_6$, $\beta_1$, $\beta_3$, $\beta_4$, and $\beta_5$) are statistically insignificant.

```{r, fig.asp = 0.4}
a.65 <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(6, 5)),  mean.model = list(armaOrder = c(5, 6), include.mean = FALSE),  distribution.model = "norm")
m.56_65 <- ugarchfit(spec = a.65, data = bitcoin.r, out.sample = 100)
m.56_65@fit$matcoef  
infocriteria(m.56_65)
arima.garch.residual.test(m.56_65, 34)
```

Before proceeding with model selection, we choose to drop candidate models that do not perform well in residual diagnostic checking. GARCH(3,2) has 3 lags present in the ACF plot of residuals representing an autoregressive component still present within this candidate model. We therefore decide to drop this candidate model from further analysis.

### AIC Ranking

The final tool for model selection involves calculating the AIC score for the ARIMA/GARCH candidate models. Ranking the scores in descending order, the most suitable model is selected with the lowest score.

```{r}
sort.aic.garch(c('m.56_22', 'm.56_33', 'm.56_44', 'm.56_54', 'm.56_55', 'm.56_65'))
```

## Final GARCH Model Chosen

Sorting the AIC scores we can see that the GARCH(5,5) has the lowest AIC of -3.776510. The model also possesses only 1 lag late within the ACF of residuals, therefore indicating that it sufficiently captures the autocorrelation in the series.

### Plot of Estimated Conditional Variances

In Figure 35B, we have plotted the conditional variance estimated by the ARIMA(5,6) + GARCH(5,5) model. We compare this to the plot of the returns series in Figure 35A to see how well it reflects the changes in variance. We can see the volatility clustering midway through 2013 is well-reflected in the conditional variance plot. Further, volatility clustering in both plots is evident between 2017-2019. This provides evidence that the model satisfactorily captures the conditional variance of the time series. 

```{r}
par(mfrow=c(2,1))

plot(diff(bitcoin.log),type='l',ylab='Conditional Variance',xlab='t',main="Figure 35A: Estimated Conditional Variances of the Daily Returns", cex.main=0.8)

plot(ts((fitted(m.56_55)[,1])^2, start = c(2013, 4, 27), freq = 365),type='l',ylab='Conditional Variance',xlab='t',main="Figure 35B: Estimated Conditional Variances of the Daily Returns", cex.main=0.8)
```



# 6. Forecasting Bitcoin Prices using ARIMA(5,1,6) + GARCH(5,5)

In forecasting,the accuracy of a model can be measured by the mean absolute scaled error (MASE). We will capture the forecasted values from the model and calculate the MASE to compare them to the real values in the next ten time points. Additionally, we will estimate the MASE of the values of the fitted model against the original time series. Together, these two MASE scores will provide an indication of the overall accuracy of the model. 

To do this we first create a dataframe for capturing the scores. The observed (real) Bitcoin values for the forecast period is imported.

```{r}
# Preparation for MASE
forecast_doc <- read_excel ('mase_tools/Bitcoin_Prices_Forecasts.xlsx')
observed <- forecast_doc$`Closing price`
mase_results <- as.data.frame(matrix(nrow=2, ncol=2))
colnames(mase_results) <- c("", "BEST MASE")
```

The chosen ARIMA/ GARCH model is fitted using the chosen orders (ARIMA(5,1,6) x GARCH(5,5)). The model is fitted using the differenced and log transformed Bitcoin series.

```{r warning=FALSE}
# Fitting the ARIMA/ GARCH model
model <- ugarchspec(variance.model = list(model = "sGARCH", garchOrder = c(5,5), submodel = NULL, external.regressors = NULL, variance.targeting = FALSE), mean.model =   list(armaOrder = c(5,6), external.regressors = NULL, distribution.model = "norm", start.pars = list(), fixed.pars = list()))
m.fit <- ugarchfit(spec = model, data = diff(log(bitcoin.ts)), solver.control = list(trace = 0))
```




**MASE over fitted values**

Capturing the fitted values, we must inverse the first difference (using the log of the first Bitcoin value in the xi parameter). Once the inverse differenced values are captured, the exponent of these values are also executed, thus successfully reversing the log transformation in the values as well. Running the MASE function with the original time series values and the untransformed, inversed differenced fitted values for the ARIMA/ GARCH model, we receive a MASE score of 19.43986 for the fitted values.

```{r warning=FALSE}
fitted.values <- fitted(m.fit)

# Inversing the difference for the fitted values
fitted.values.invdiff <- diffinv(as.vector(fitted.values), xi=log(bitcoin.ts[1]))

# Reversing the log transformation for the fitted values
fitted.values.invdiff.exp <- exp(fitted.values.invdiff)

# MASE of fitted values
mase_results[1,] <- c("Over Fitted Values",MASE(bitcoin.ts,fitted.values.invdiff.exp)$MASE$MASE)
```



**MASE over the forecasts**

Capturing the forecasted values, we again inverse the first difference (using the log of the last Bitcoin value in the xi parameter). After this, we yet again capture the exponent of these values. We run these values with the supplied real values for the forecast period to get a MASE score of 1.26147688.

```{r warning=FALSE}
forc <- ugarchforecast(m.fit, n.ahead = 10 ,data = diff(log(bitcoin.ts)))
plot(forc, which = 1)
forecasts <- forc@forecast$seriesFor

# Inversing the difference for the forecast values
forecasts.invdiff <- diffinv(as.vector(forecasts), xi=log(bitcoin.ts[length(bitcoin.ts)]))

# Reversing the log transformation for the forecast values
forecasts.invdiff.exp <- exp(forecasts.invdiff)

# MASE of forecast values
mase_results[2,] <- c("Over Forecasts",MASE(observed,forecasts.invdiff.exp[1:10])$MASE$MASE)
```


**Plot original timeseries with inverse differenced and untransformed forecasts**

The plot of the inversed differenced and untransformed forecast is plotted below with the original time series. The confidence intervals are generated by capturing the sigmas (standard deviation) of the forecast values, and also by reversing the differencing and log transformations that are inherently captured in these sigma values. The upper and lower bounds for the forecasts are then calculated and turned into a polygon shape outlining the confidence intervals for the forecast Figure 36B shows a zoomed in plot so the forecast line and confidence intervals can be seen clearly. The zoomed in series displays the series from August 2018.

```{r}
plot.reverse.forecast(bitcoin.ts, forc, n.ahead=10, title="Bitcoin time series", zoomed.date = 2018.8, fig_num = 36)
```

\newpage

# 7. MASE

The overall MASE results are therefore as follows: 

```{r}
mase_results
```

The MASE estimation of 1.2614 for the 10-day forecast is quite close to zero, so we concluded that our fitted model appears to provide accurate forecasts, at least in the very short term. The forecast suggests that the daily closing price of Bitcoin will remain fairly stable. In financial trading, stable periods are considered low-risk, so as an investor this information could be used to suggest that it is a safe time to buy or sell Bitcoin stock.

The MASE for the fitted model value against the entire back series is 19.4399, much further away from zero than the forecast MASE. However, considering the entire series contains over 2,000 observations and several periods of severe volatility clustering, we concluded that our model performed quite well.

# Conclusion

To arrive at the chosen model, we firstly inspected the time series plot and identified that it exhibited trend, autocorrelation and changing variance. We confirmed autocorrelation and decided to continue fitting the data with an ARIMA model. We first applied a log transformation and took the first difference to the data in order to render it stationary. Once stationarity was confirmed, we used a variety of methods to estimate potential orders of the model. Out of the possible models, we used residual diagnostic checking and statistical testing and found sufficient evidence to choose ARIMA(5,1,6) as our model. We saw evidence of conditional volatility and confirmed this using residual analysis and statistical testing. We then took the absolute- and square-transformed residuals to estimate possible orders of a GARCH component. The estimated orders were subjected to diagnostic checking and statistical testing. We found sufficient justification to select GARCH(5,5) and created a combined ARIMA + GARCH model.

Through diagnostic checking and statistical tests, we concluded that the best model for fitting the Bitcoin Daily Prices time series was ARIMA(5,1,6) + GARCH(5,5). However, we recognise there are limitations on the model’s ability to capture the entire autocorrelation and variance structure of the data. This is evident in the residual analysis where the QQ Plot still shows thick tails deviating away from the line of normal distribution. In addition, the Ljung-Box test for this model on the standardised residuals showed low p-values, indicating there may be some serial autocorrelation we have not captured, even though we saw no significant lags in the ACF. This is evidence that our residuals may not be independent, identically distributed random variables. For such a complex and volatile series as Bitcoin Daily Prices, it is highly unlikely that we would find a model that perfectly captured the nature of the entire series.

In general, financial time series are difficult to model because they are sensitive to exogenous shocks. As a result, variance is not consistent over time, and is conditional on particular time periods in the series. By including a GARCH component in our chosen model, we attempted to account for this conditional volatility, in addition to modelling changes in mean using ARIMA. The GARCH extension, rather than the ARCH extension, assumes that distant volatility has less influence on current volatility than recent lags. This was an appropriate choice for our modelling, given the first half of the Bitcoin Daily Prices series had very low variance initially, which drastically increased in the second half of the series.

Because of the complexity of this series, we recognised some ambiguities in the interpretation of statistical outputs. For example, when deciding on an ARIMA model, the AIC and BIC resulted in different choices for the optimal model. We took into account a number of indicators to justify our modelling decisions and to look for models with as much consistency in the residual analysis and statistical tests as possible. However, it is entirely possible that alternative models could be justified for this series. We feel confident that our decision-making process and model selection is supported by the satisfactory performance of the MASE score for both the fitted series and the forecast.


# References

Bambrough, B 2017, Blow To Bitcoin As Mark Zuckerberg Warns Facebook Payments Are Coming, <https://www.forbes.com/sites/billybambrough/2019/03/07/blow-to-bitcoin-as-mark-zuckerberg-warns-facebook-payments-are-coming/#55eaa8566daa>

Bambrough, B 2018, Google Has Suddenly Scrapped Its Bitcoin Ad Ban -- Here's What That Means, <https://www.forbes.com/sites/billybambrough/2018/09/25/google-is-scraping-its-bitcoin-ban-heres-what-that-will-mean/#28aaa60e5654>

Chohan, U W 2017, Assessing the Differences in Bitcoin & Other Cyptocurrency Legality Across National Jurisdictions, UNSW Business School, pp.1-11

Yaziz A R, Azizan N A, Zakiria R, Alumad M H, 2013, The performance of hybrid ARIMA-GARCH modeling in forecasting gold price, Faculty of Industrial Sciences and Technology, Universiti Malaysia Pahang, p. 1202-1203

Grachev, O 2017, Application of Time Series Models (ARIMA, GARCH, and ARMA-GARCH) for Stock Market Forecasting, Northern Illinois University  Department of Economics, p.15

Petrica A C, Stancu S, Tindeche A, 2016, Limitation of ARIMA models in financial and monetary economics, Vol 23, No. 4, Winter, Theoretical and Applied Economics, pp. 19-42

Statement of Jennifer Shasky Calvery, Director Financial Crimes Enforcement Network United States Department of the Treasury, Nov 18, 2013



# Appendix

#### Appendix 1.1

A function that plots the ACF and PACF tests side by side.

```{r}
acf.pacf.f <- function(x, title1, title2) {
  par(mfrow = c(1,2))
  acf(x, main = title1, cex.main=0.8)
  pacf(x, main = title2, cex.main=0.8)
  par(mfrow = c(1,1))}
```

#### Appendix 1.2

A function that compares any two objects in a side by side plot (one plot above and one plot below).

```{r}
plot.compare.f <- function(x1, x2, title1, title2){
  par(mfrow=c(2,1))
  plot(x1, type = "l", main = title1, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  plot(x2, type = "l", main = title2, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  par(mfrow=c(1,1))
}
```


#### Appendix 1.3

A function executing all tests for normality (histogram, QQ plots and the Shapiro Wilks test).

```{r}
norm.f <- function(model, titlehist, titleqq)
         {par(mfrow=c(1,2))
            hist(model, main = titlehist, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
            qqnorm(model, main = titleqq, cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
            qqline(model, col = 2, lwd = 1, lty = 2)
            print(shapiro.test(model))
            par(mfrow=c(1,1))}
```

#### Appendix 1.4

A function that groups together the ACF, PACF, EACF and BIC tests in order to estimate canidate p and q orders.

```{r}
p.q.orders <- function(x, titleA, titleP, ar.max, ma.max, nar, nma)
              {acf.pacf.f(x, titleA, titleP)
              eacf(x, ar.max = ar.max, ma.max = ma.max)
              res = armasubsets(y = x, nar = nar, nma = nma, ar.method = 'yule-walker')
              plot(res)}
```

#### Appendix 1.5

A function that groups all tests required as a diagnostic tool for residuals of an ARIMA modelling process.

```{r}
arima.test <- function(model, fig_num){
  res <- rstandard(model)
  par(mfrow = c(2,2))
  plot(res, type = 'o', ylab = 'Standardised residuals', main = paste0("Figure ",as.character(fig_num),"A: Time series plot of standardised residuals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  abline(h = 0)
  hist(res, main = paste0("Figure ",as.character(fig_num),"B: Histogram of standardised residuals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  qqnorm(res, main = paste0("Figure ",as.character(fig_num),"C: QQPlot of standardised residuals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  qqline(res, col = 2, lwd = 1, lty = 2)
  acf(res, main = paste0("Figure ",as.character(fig_num),"D: ACF of the Standardised Resiudals"), cex.main=0.8, cex.axis=0.8, cex.lab=0.8)
  par(mfrow = c(1,1))
  tsdiag(model, gof = 15, omit.initial = F, main="E:")
  print(shapiro.test(res))
}
```

#### Appendix 1.6

A function that groups all plots required as a diagnostic tool for residuals of an ARIMA/ GARCH modelling process.

```{r}
arima.garch.residual.test <- function(model, fig_num){
  par(mfrow = c(1,2))
  par(col.main='white') 
  plot(model, which = 8)
  par(col.main='black') 
  title(paste0("Figure ",fig_num,"A: Empirical Density of Standardized Residuals"))
  par(col.main='white') 
  plot(model, which = 9)
  par(col.main='black') 
  title(paste0("Figure ",fig_num,"B: norm - QQ Plot"))
  par(mfrow = c(1,1))
  par(col.main='white') 
  plot(model, which = 10)
  par(col.main='black') 
  title(paste0("Figure ",fig_num,"C: ACF of Standardized Residuals"))
}
```

#### Appendix 1.7

A function sorts AIC/ BIC scores, as obtained from the MATH1318 Canvas page for Module 7

```{r}
sort.score <- function(x, score = c("bic", "aic")){
  if (score == "aic"){
    x[with(x, order(AIC)),]
  } else if (score == "bic") {
    x[with(x, order(BIC)),]
  } else {
    warning('score = "x" only accepts valid arguments ("aic","bic")')
  }
}
```

#### Appendix 1.8

A quick function that orders the results in a BIC table.

```{r}
sort.bic <- function(df){
  df <- df[order(df$BIC),]
  return(df)
}
```

#### Appendix 1.9

A function that capture AIC scores and sorts the scores.

```{r}
sort.aic.garch <- function(models){
  scores <- as.data.frame(matrix(nrow=length(models), ncol=2))
  colnames(scores) <- c("Model","AIC")
  for (i in 1:length(models)){ 
    scores$Model[i] <- models[i]
    scores$AIC[i] <- infocriteria(get(models[i]))[1] 
  }
  scores <- scores[order(scores$AIC),]
  rownames(scores) <- 1:nrow(scores)
  return(scores)
}
```

#### Appendix 1.10

A function that calculates the upperbound of a value using the standard deviation.

```{r}
upperbound <- function(value, sigma){
  return(value + 1.96*sqrt(sigma))
}
```


#### Appendix 1.11

A function that calculates the lowerbound of a value using the standard deviation.

```{r}
lowerbound <- function(value, sigma){
  return(value - 1.96*sqrt(sigma))
}
```


#### Appendix 1.12

A function that reverses first differencing, log transformations, captures confidence intervals of the forecast and then outputs a plot (or plots).

```{r}
plot.reverse.forecast <- function(t.series, forc, n.ahead, title = NULL, zoomed.date = NULL, fig_num = NULL){
  
  forecasts <- forc@forecast$seriesFor
  f <- frequency(t.series)
  
  # Store all forecast values
  forecast.values <- as.data.frame(matrix(nrow=(n.ahead+1), ncol=9))
  columns <- c('time','values.f', 'values.f.invd', 'values.f.invd.exp','sigma', 'sigma.invd', 'sigma.invd.exp',
             'upper.values.f', 'lower.values.f')
  colnames(forecast.values) <- columns
  
  # Store forecast time
  forecast.values$time <- seq(time(t.series)[length(t.series)]+(1/f), (time(t.series)[length(t.series)]+(1/f))+(n.ahead*1/f), by=(1/f))
  
  # Store forecast values
  forecast.values$values.f <- c(forecasts,NA)
  
  # Store forecast inverse difference values
  forecast.values$values.f.invd <-  diffinv(as.vector(forecasts), xi=log(t.series[length(t.series)]))
  
  # Store forecast inverse exponent values
  forecast.values$values.f.invd.exp <- exp(forecast.values$values.f.invd)
  
  # capture forecasted sigma values
  forecast.values$sigma <- c(as.vector(forc@forecast$sigmaFor),NA)
  
  # inv difference for forecasted sigma values
  forecast.values$sigma.invd <- diffinv(forecast.values$sigma[1:10], xi=log(sd(t.series)))
  
  # exp for forecasted sigma values
  forecast.values$sigma.invd.exp <- exp(forecast.values$sigma.invd)
  
  # Upper and lower values of forecasted values
  forecast.values$upper.values.f <- upperbound(forecast.values$values.f.invd.exp, forecast.values$sigma.invd.exp)
  forecast.values$lower.values.f <- lowerbound(forecast.values$values.f.invd.exp, forecast.values$sigma.invd.exp)
  
  # Turn forecast values into their own timeseries
  forecast.ts <- ts(forecast.values$values.f.invd.exp[1:10], start=(time(t.series)[length(t.series)]+(1/f)),frequency = f)

  # Plot Normal series  
  if (f == 365){
    period = 'day'
  } else if (f == 12) {
    period = 'month'
  } else if (f == 4) {
    period = 'quarter'
  } else {
    period = 'period'
  }
  
  plot(x = t.series, xlim=c(time(t.series)[1],time(t.series)[length(t.series)]+((n.ahead)/f)), ylab="", xlab="Year", main=paste0("Figure ",fig_num," A: ",title, " with ",n.ahead," ",period," forecast"), type="l", cex.main=0.8)
  lines(forecast.ts, col="red", type="l")
  polygon(c((forecast.values$time[1:10]),rev(forecast.values$time[1:10])), c(forecast.values$upper.values.f[1:10],rev(forecast.values$lower.values.f[1:10])), col=rgb(0,0,0.6,0.2), border=FALSE)
  
  if (!is.null(zoomed.date)){
     # Plot Zoomed Series
    plot(x = t.series, xlim=c(zoomed.date,time(t.series)[length(t.series)]+((n.ahead)/365)), ylim=c(3500,4200), ylab="", xlab="Year", main=paste0("Figure ", fig_num," B: ",title," with ",n.ahead," ", period, " forecast ", "(zoomed plot commencing at " , zoomed.date,")"), type="l", cex.main=0.8)
    lines(forecast.ts, col="red", type="l")
    polygon(c((forecast.values$time[1:10]),rev(forecast.values$time[1:10])),
            c(forecast.values$upper.values.f[1:10],rev(forecast.values$lower.values.f[1:10])), col=rgb(0,0,0.6,0.2),
            border=FALSE)
  }
} 
```

#### Appendix 1.14

A supplied function for calculation MASE.

```{r}
MASE <- function(observed , fitted ){
  # observed: Observed series on the forecast period
  # fitted: Forecast values by your model
  Y.t = observed
  n = length(fitted)
  e.t = Y.t - fitted
  sum = 0 
  for (i in 2:n){
    sum = sum + abs(Y.t[i] - Y.t[i-1] )
  }
  q.t = e.t / (sum/(n-1))
  MASE = data.frame( MASE = mean(abs(q.t)))
  return(list(MASE = MASE))
}
```



